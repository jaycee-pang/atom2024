{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf456674-bc2e-47f4-8898-7c91d24d510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/cvnnx9qn3tj18cq5_9wx39xm0000gn/T/ipykernel_42404/2916274574.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, matthews_corrcoef, balanced_accuracy_score, confusion_matrix, f1_score, roc_curve,precision_recall_curve, auc\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/')\n",
    "from RF_GSCV import * # RF_GSCV contains the calculate metrics function to get the TP, TN, FP, FN scores \n",
    "from RF_atomver import prediction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379ec7ea-fcfa-4e95-b240-791ec2db8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    # def __init__(self, train_x, train_y, likelihood):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2d38620-904f-4ffb-aa50-ce6beabf1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer: \n",
    "    def __init__(self,model, likelihood, iterations): \n",
    "        self.model = model\n",
    "        self.likelihood = likelihood \n",
    "        smoke_test = ('CI' in os.environ)\n",
    "        self.n_iterations = 2 if smoke_test else iterations\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "        self.loss_fn = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "    def train(self, train_x, train_y): \n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "        predictions = [] \n",
    "        for i in range(self.n_iterations): \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(train_x)\n",
    "            loss = -self.loss_fn(output, self.likelihood.transformed_targets).sum()\n",
    "            # loss = -self.loss_fn(output, train_y).sum()\n",
    "            loss.backward()\n",
    "            if (i%10==0): \n",
    "                print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                    i + 1, self.n_iterations, loss.item(),\n",
    "                    self.model.covar_module.base_kernel.lengthscale.mean().item(),\n",
    "                    self.model.likelihood.second_noise_covar.noise.mean().item()\n",
    "                ))\n",
    "             \n",
    "            self.optimizer.step() \n",
    "\n",
    "    def predict(self, input): \n",
    "        \"\"\"\n",
    "        Make predictions using the GP model.\n",
    "        Args:\n",
    "            input (torch.Tensor): The input data for making predictions.\n",
    "        \n",
    "        Returns:\n",
    "            dist (gpytorch.distributions.MultivariateNormal): The distribution representing the GP posterior.\n",
    "            observed_pred (gpytorch.distributions.MultivariateNormal): The predicted distribution considering the likelihood.\n",
    "            pred_means (torch.Tensor): The means of the predicted distributions.\n",
    "            class_pred (torch.Tensor): The predicted class labels.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "            dist = self.model(input)     # output distribution\n",
    "            pred_means = dist.loc          # means of distributino \n",
    "            observed_pred = self.likelihood(self.model(input))    # likelihood predictions mean and var  \n",
    "\n",
    "            class_pred = self.model(input).loc.max(0)[1]\n",
    "            \n",
    "        return dist, observed_pred, pred_means, class_pred\n",
    "\n",
    "    def evaluate(self, x_input, y_true): \n",
    "        \"\"\"\n",
    "        Evaluate the GP model.\n",
    "        Args:\n",
    "            x_input (torch.Tensor): The input data features.\n",
    "            y_true (torch.Tensor): The true labels for the input data.\n",
    "        Returns:\n",
    "            y_pred (numpy.ndarray): The predicted class labels.\n",
    "        \"\"\"\n",
    "        y_pred = self.model(x_input).loc.max(0)[1].numpy()\n",
    "        return y_pred\n",
    "\n",
    "    def gp_results(self, x_input, y_true, plot_title=None): \n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics and print results.\n",
    "        Args:\n",
    "            x_input (torch.Tensor): The input data features.\n",
    "            y_true (torch.Tensor or numpy.ndarray): The true labels for the input data.\n",
    "            plot_title (str, optional): The title for the confusion matrix plot.\n",
    "        Returns:\n",
    "            dict: A dictionary containing evaluation metrics and confusion matrix components.\n",
    "        \"\"\"\n",
    "        y_pred = self.evaluate(x_input, y_true) \n",
    "        if isinstance(y_true, torch.Tensor):\n",
    "            y_true = y_true.numpy().reshape(-1)\n",
    "        # plot_confusion_matrix(y_true, y_pred, ['0','1'], title=plot_title)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        dist = self.model(x_input)     # get predicted distributions \n",
    "        pred_means = dist.loc          # means for predicted dist  \n",
    "\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        tp, tn, fp, fn = calculate_metrics(y_true, y_pred) \n",
    "        sensitivity = tp / (tp + fn) \n",
    "        specificity = tn / (tn + fp) \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(f'accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, specificity: {specificity:.4f}, cm: {cm}')\n",
    "        return {'accuracy': accuracy, 'precision': precision,  'recall':recall, 'specificity':specificity, 'TN': tn, 'FN': fn, 'FP': fp, 'TP': tp }\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03196461-714f-41ad-ab58-67c40014acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_torch_tens_float(filepath, filename): \n",
    "    trainX_df = pd.read_csv(filepath+filename+'_trainX.csv')\n",
    "    trainy_df = pd.read_csv(filepath+filename+'_train_y.csv')\n",
    "    testX_df = pd.read_csv(filepath+filename+'_testX.csv')\n",
    "    testy_df = pd.read_csv(filepath+filename+'_test_y.csv')\n",
    "\n",
    "    train_x_temp = trainX_df.to_numpy().astype(\"float32\")\n",
    "    test_x_temp = testX_df.to_numpy().astype(\"float32\")\n",
    "    \n",
    "    train_y_temp = trainy_df.to_numpy().flatten().astype(\"long\")\n",
    "    test_y_temp = testy_df.to_numpy().flatten().astype(\"long\")\n",
    "    \n",
    "    trainX = torch.from_numpy(train_x_temp)\n",
    "    trainy = torch.from_numpy(train_y_temp)\n",
    "    testX = torch.from_numpy(test_x_temp)\n",
    "    testy = torch.from_numpy(test_y_temp)\n",
    "    print(f'train X: {trainX.shape}, train y: {trainy.shape}, test X: {testX.shape}, test y: {testy.shape}')\n",
    "    return trainX, trainy, testX, testy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6faad9bf-cb3a-4e74-87d2-f8fe148378aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results(trainX, trainy, testX, testy, root_name, n_iterations=300, n_samples=100):\n",
    "    \"\"\"\n",
    "    Train a Dirichlet Gaussian Process model and save the training and test performance results.\n",
    "    EXAct ExactGPModel\n",
    "    This function trains a Dirichlet GP model on the given training data, evaluates it on both the training\n",
    "    and test data, and saves various performance metrics and predictions to pandas DataFrames.\n",
    "\n",
    "    Args:\n",
    "        trainX (torch.Tensor): The training data features.\n",
    "        trainy (torch.Tensor): The training data labels.\n",
    "        testX (torch.Tensor): The test data features.\n",
    "        testy (torch.Tensor): The test data labels.\n",
    "        root_name (str): The root name used for labeling the model in the results.\n",
    "        n_iterations (int, optional): The number of training iterations. Default is 300.\n",
    "        n_samples (int, optional): The number of samples for prediction. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "        train_perf_df (pd.DataFrame): DataFrame containing performance metrics and predictions for the training data.\n",
    "        test_perf_df (pd.DataFrame): DataFrame containing performance metrics and predictions for the test data.\n",
    "    \"\"\"\n",
    "    likelihood = DirichletClassificationLikelihood(trainy, learn_additional_noise=True)\n",
    "    # likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(trainX, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)\n",
    "    # model = ExactGPModel(trainX, trainy, likelihood)\n",
    "    # n_iterations = 300\n",
    "    trainer = Trainer(model, likelihood, n_iterations)\n",
    "    trainer.train(trainX, trainy) \n",
    "  \n",
    "    train_dist, train_observed_pred, train_pred_means, train_pred  = trainer.predict(trainX)\n",
    "    train_results = trainer.gp_results(trainX, trainy)\n",
    "    test_dist, test_observed_pred, test_pred_means, test_pred  = trainer.predict(testX)\n",
    "    test_results = trainer.gp_results(testX, testy)\n",
    "    \n",
    "    train_observed_pred.mean.numpy()\n",
    "    train_pred_variance2D = train_observed_pred.variance.numpy()\n",
    "    test_observed_pred.mean.numpy()\n",
    "    test_pred_variance2D=test_observed_pred.variance.numpy()\n",
    "    \n",
    "    train_pred_samples = train_dist.sample(torch.Size((256,))).exp()\n",
    "    train_probabilities = (train_pred_samples / train_pred_samples.sum(-2, keepdim=True)).mean(0)\n",
    "\n",
    "    train_prob_stds = (train_pred_samples / train_pred_samples.sum(-2, keepdim=True)).std(0)\n",
    "\n",
    "    test_pred_samples = test_dist.sample(torch.Size((100,))).exp()\n",
    "\n",
    "    test_probabilities = (test_pred_samples / test_pred_samples.sum(-2, keepdim=True)).mean(0)\n",
    "    test_prob_stds = (test_pred_samples / test_pred_samples.sum(-2, keepdim=True)).std(0)\n",
    "\n",
    " \n",
    "    train_perf_df = pd.DataFrame()\n",
    "    test_perf_df = pd.DataFrame()\n",
    "    train_perf_df['mean_pred_class0'] = train_observed_pred.mean.numpy()[0,]\n",
    "    train_perf_df['mean_pred_class1'] = train_observed_pred.mean.numpy()[1,]\n",
    "    train_perf_df['y'] = trainy\n",
    "    train_perf_df['y_pred'] = train_pred_means.max(0)[1]\n",
    "    train_perf_df['var_pred_class0']=train_observed_pred.variance.numpy()[0,]\n",
    "    train_perf_df['var_pred_class1']=train_observed_pred.variance.numpy()[1,]\n",
    "    train_perf_df['pred_prob_class0'] = train_probabilities.numpy()[0,]\n",
    "    train_perf_df['pred_prob_class1'] = train_probabilities.numpy()[1,]\n",
    "    train_perf_df['pred_prob_std_class0'] = train_prob_stds.numpy()[0,]\n",
    "    train_perf_df['pred_prob_std_class1'] = train_prob_stds.numpy()[1,]\n",
    "    train_perf_df['model'] = f'{root_name}_GP'\n",
    "    train_perf_df['subset'] = 'train' \n",
    "    train_cm = confusion_matrix(trainy, train_perf_df['y_pred'])\n",
    "    cm_flattened = train_cm.flatten().tolist()\n",
    "    train_perf_df['cm']= [cm_flattened]* len(train_perf_df)\n",
    "    train_perf_df['prediction_type'] = train_perf_df.apply(lambda x: prediction_type(x['y'], x['y_pred']), axis=1)\n",
    "    train_perf_df['ROC-AUC'] = roc_auc_score(trainy, train_perf_df['y_pred'])\n",
    "    train_perf_df['MCC'] = matthews_corrcoef(trainy, train_perf_df['y_pred'])\n",
    "    train_perf_df['Balanced Accuracy'] = balanced_accuracy_score(trainy, train_perf_df['y_pred'])\n",
    "    train_perf_df['f1'] = f1_score(trainy, train_perf_df['y_pred'])\n",
    "    \n",
    "    test_perf_df['mean_pred_class0'] = test_observed_pred.mean.numpy()[0,]\n",
    "    test_perf_df['mean_pred_class1'] = test_observed_pred.mean.numpy()[1,]\n",
    "    test_perf_df['y'] = testy\n",
    "    test_perf_df['y_pred'] = test_pred_means.max(0)[1]\n",
    "    test_perf_df['var_pred_class0']=test_observed_pred.variance.numpy()[0,]\n",
    "    test_perf_df['var_pred_class1']=test_observed_pred.variance.numpy()[1,]\n",
    "    test_perf_df['pred_prob_class0'] = test_probabilities.numpy()[0,]\n",
    "    test_perf_df['pred_prob_class1'] = test_probabilities.numpy()[1,]\n",
    "    test_perf_df['pred_prob_std_class0'] =test_prob_stds.numpy()[0,]\n",
    "    test_perf_df['pred_prob_std_class1'] = test_prob_stds.numpy()[1,]\n",
    "    test_perf_df['model'] = f'{root_name}_GP_exactGP'\n",
    "    test_perf_df['subset'] = 'test' \n",
    "    test_cm = confusion_matrix(testy, test_perf_df['y_pred'])\n",
    "    test_cm_flattened = test_cm.flatten().tolist()\n",
    "    test_perf_df['cm']= [test_cm_flattened]* len(test_perf_df)\n",
    "    test_perf_df['prediction_type'] = test_perf_df.apply(lambda x: prediction_type(x['y'], x['y_pred']), axis=1)\n",
    "    test_perf_df['ROC-AUC'] = roc_auc_score(testy, test_perf_df['y_pred'])\n",
    "    test_perf_df['MCC'] = matthews_corrcoef(testy, test_perf_df['y_pred'])\n",
    "    test_perf_df['Balanced Accuracy'] = balanced_accuracy_score(testy, test_perf_df['y_pred'])\n",
    "    test_perf_df['f1'] = f1_score(testy, test_perf_df['y_pred'])\n",
    "    with open(f'{GP_holdout}{root_name}_ExactGP_model.pkl', 'wb') as f: \n",
    "        pickle.dump(model,f)\n",
    "    with open(f'{GP_holdout}{root_name}_GP_same_Dirichlet_likelihood.pkl', 'wb') as f: \n",
    "        pickle.dump(likelihood,f)\n",
    "    for k, val in train_results.items(): \n",
    "        train_perf_df[k] = val\n",
    "    for k, val in test_results.items():\n",
    "        test_perf_df[k] = val\n",
    "    return train_perf_df, test_perf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb7b782a-0791-480d-8a34-cc415cb653eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEK2\n",
      "NEK2 binding moe scaled\n",
      "train X: torch.Size([1125, 306]), train y: torch.Size([1125]), test X: torch.Size([283, 306]), test y: torch.Size([283])\n",
      "trainX:torch.Size([1125, 306]), train y: torch.Size([1125]), testX: torch.Size([283, 306]), test y: torch.Size([283])\n",
      "Iter 1/300 - Loss: 7.107   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 5.796   lengthscale: 1.301   noise: 0.812\n",
      "Iter 21/300 - Loss: 5.307   lengthscale: 2.096   noise: 1.092\n",
      "Iter 31/300 - Loss: 4.786   lengthscale: 3.160   noise: 1.364\n",
      "Iter 41/300 - Loss: 4.316   lengthscale: 4.420   noise: 1.505\n",
      "Iter 51/300 - Loss: 4.112   lengthscale: 5.463   noise: 1.517\n",
      "Iter 61/300 - Loss: 3.964   lengthscale: 6.214   noise: 1.457\n",
      "Iter 71/300 - Loss: 3.826   lengthscale: 6.801   noise: 1.366\n",
      "Iter 81/300 - Loss: 3.741   lengthscale: 7.334   noise: 1.275\n",
      "Iter 91/300 - Loss: 3.684   lengthscale: 7.866   noise: 1.201\n",
      "Iter 101/300 - Loss: 3.635   lengthscale: 8.377   noise: 1.146\n",
      "Iter 111/300 - Loss: 3.607   lengthscale: 8.836   noise: 1.099\n",
      "Iter 121/300 - Loss: 3.579   lengthscale: 9.245   noise: 1.059\n",
      "Iter 131/300 - Loss: 3.552   lengthscale: 9.624   noise: 1.028\n",
      "Iter 141/300 - Loss: 3.538   lengthscale: 9.982   noise: 1.010\n",
      "Iter 151/300 - Loss: 3.515   lengthscale: 10.321   noise: 1.000\n",
      "Iter 161/300 - Loss: 3.509   lengthscale: 10.642   noise: 0.995\n",
      "Iter 171/300 - Loss: 3.493   lengthscale: 10.948   noise: 0.993\n",
      "Iter 181/300 - Loss: 3.481   lengthscale: 11.243   noise: 0.996\n",
      "Iter 191/300 - Loss: 3.472   lengthscale: 11.527   noise: 1.002\n",
      "Iter 201/300 - Loss: 3.459   lengthscale: 11.801   noise: 1.008\n",
      "Iter 211/300 - Loss: 3.453   lengthscale: 12.067   noise: 1.014\n",
      "Iter 221/300 - Loss: 3.445   lengthscale: 12.326   noise: 1.020\n",
      "Iter 231/300 - Loss: 3.439   lengthscale: 12.577   noise: 1.024\n",
      "Iter 241/300 - Loss: 3.428   lengthscale: 12.824   noise: 1.031\n",
      "Iter 251/300 - Loss: 3.426   lengthscale: 13.064   noise: 1.037\n",
      "Iter 261/300 - Loss: 3.419   lengthscale: 13.298   noise: 1.042\n",
      "Iter 271/300 - Loss: 3.412   lengthscale: 13.527   noise: 1.045\n",
      "Iter 281/300 - Loss: 3.404   lengthscale: 13.752   noise: 1.048\n",
      "Iter 291/300 - Loss: 3.403   lengthscale: 13.973   noise: 1.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1125]' is invalid for input of size 2250",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m trainX, trainy, testX, testy \u001b[38;5;241m=\u001b[39m make_torch_tens_float(nek_path,file_root)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainX:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainy\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, testX: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test y: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtesty\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m train_perf_df, test_perf_df \u001b[38;5;241m=\u001b[39m \u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesty\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfile_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m train_perf_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEK\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEK\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnek\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     27\u001b[0m train_perf_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrategy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m, in \u001b[0;36msave_results\u001b[0;34m(trainX, trainy, testX, testy, root_name, n_iterations, n_samples)\u001b[0m\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, likelihood, n_iterations)\n\u001b[1;32m     27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(trainX, trainy) \n\u001b[0;32m---> 29\u001b[0m train_dist, train_observed_pred, train_pred_means, train_pred  \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m train_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mgp_results(trainX, trainy)\n\u001b[1;32m     31\u001b[0m test_dist, test_observed_pred, test_pred_means, test_pred  \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(testX)\n",
      "Cell \u001b[0;32mIn[18], line 45\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gpytorch\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mfast_pred_var(), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 45\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m     \u001b[38;5;66;03m# output distribution\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     pred_means \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mloc          \u001b[38;5;66;03m# means of distributino \u001b[39;00m\n\u001b[1;32m     47\u001b[0m     observed_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28minput\u001b[39m))    \u001b[38;5;66;03m# likelihood predictions mean and var  \u001b[39;00m\n",
      "File \u001b[0;32m~/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:336\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     (\n\u001b[1;32m    331\u001b[0m         predictive_mean,\n\u001b[1;32m    332\u001b[0m         predictive_covar,\n\u001b[1;32m    333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_strategy\u001b[38;5;241m.\u001b[39mexact_prediction(full_mean, full_covar)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m \u001b[43mpredictive_mean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtest_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m full_output\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(predictive_mean, predictive_covar)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1125]' is invalid for input of size 2250"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/'\n",
    "samplings = ['scaled', 'UNDER', 'SMOTE', 'ADASYN'] \n",
    "feat_types = ['moe', 'mfp']\n",
    "neks = ['2', '3', '5', '9']\n",
    "GP_path= '/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/GP/GP_matern_kernel/' \n",
    "for nek in neks:\n",
    "    print(f'NEK{nek}')\n",
    "    bind_inhib = ['binding', 'inhibition']\n",
    "    if nek in ['3','5']: \n",
    "        bind_inhib = ['binding']\n",
    "    else: \n",
    "        bind_inhib = ['binding', 'inhibition']\n",
    "    for bi in bind_inhib: \n",
    "        if bi == 'binding': \n",
    "            this_bi = 'bind' \n",
    "        if bi == 'inhibition': \n",
    "            this_bi = 'inhib'\n",
    "        nek_path= f'{data_dir}NEK{nek}/{this_bi}/'\n",
    "        for feat in feat_types: \n",
    "            for samp in samplings: \n",
    "                print(f'NEK{nek} {bi} {feat} {samp}')\n",
    "                file_root = f'NEK{nek}_{bi}_{feat}_{samp}'\n",
    "                trainX, trainy, testX, testy = make_torch_tens_float(nek_path,file_root)\n",
    "                print(f'trainX:{trainX.shape}, train y: {trainy.shape}, testX: {testX.shape}, test y: {testy.shape}')\n",
    "                train_perf_df, test_perf_df = save_results(trainX, trainy, testX, testy,file_root, n_iterations=300)\n",
    "                train_perf_df['NEK'] = f'NEK{nek}_{bi}'\n",
    "                train_perf_df['strategy'] = f'{samp}'\n",
    "                train_perf_df['feat_type'] = f'{feat}'\n",
    "                test_perf_df['NEK'] = f'NEK{nek}_{bi}'\n",
    "                test_perf_df['strategy'] = f'{samp}'\n",
    "                test_perf_df['feat_type'] = f'{feat}'\n",
    "                train_perf_df.to_csv(f'{GP_path}{file_root}_train_GP_matern.csv',index=False) \n",
    "                test_perf_df.to_csv(f'{GP_path}{file_root}_test_GP_matern.csv',index=False) \n",
    "                print()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39f3e6-478f-49de-b290-1856e07b7420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958505ef-7fd7-4494-a3df-5ffc1298d0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch-venv",
   "language": "python",
   "name": "gpytorch-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
