{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf456674-bc2e-47f4-8898-7c91d24d510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/cvnnx9qn3tj18cq5_9wx39xm0000gn/T/ipykernel_1184/2293997119.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, matthews_corrcoef, balanced_accuracy_score, confusion_matrix, f1_score, roc_curve,precision_recall_curve, auc\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/')\n",
    "from RF_GSCV import * # RF_GSCV contains the calculate metrics function to get the TP, TN, FP, FN scores \n",
    "from RF_atomver import prediction_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379ec7ea-fcfa-4e95-b240-791ec2db8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DirichletGPModel(ExactGP):\n",
    "    \"\"\"\n",
    "    A Dirichlet Gaussian Process (GP) model for multi-class classification.\n",
    "    This model uses a Gaussian Process with a Dirichlet prior to handle multi-class classification tasks.\n",
    "    It extends the ExactGP class from GPyTorch, a library for Gaussian Processes in PyTorch.\n",
    "    Attributes:\n",
    "        mean_module (gpytorch.means.ConstantMean): The mean module for the GP, initialized with a constant mean function for each class.\n",
    "        covar_module (gpytorch.kernels.ScaleKernel): The covariance module for the GP, using a scaled RBF kernel for each class.\n",
    "\n",
    "    Args:\n",
    "        train_x (torch.Tensor): Training data features.\n",
    "        train_y (torch.Tensor): Training data labels.\n",
    "        likelihood (gpytorch.likelihoods.Likelihood): The likelihood function.\n",
    "        num_classes (int): The number of classes for the classification task.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = ScaleKernel(MaternKernel(nu=0.5, batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the GP model.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data features.\n",
    "        Returns:\n",
    "            gpytorch.distributions.MultivariateNormal: The multivariate normal distribution representing the GP posterior.\n",
    "        \"\"\"\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d38620-904f-4ffb-aa50-ce6beabf1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer: \n",
    "    def __init__(self,model, likelihood, iterations): \n",
    "        self.model = model\n",
    "        self.likelihood = likelihood \n",
    "        smoke_test = ('CI' in os.environ)\n",
    "        self.n_iterations = 2 if smoke_test else iterations\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "        self.loss_fn = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n",
    "        \n",
    "    def train(self, train_x, train_y): \n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "        predictions = [] \n",
    "        for i in range(self.n_iterations): \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(train_x)\n",
    "            loss = -self.loss_fn(output, self.likelihood.transformed_targets).sum()\n",
    "            loss.backward()\n",
    "            if (i%10==0): \n",
    "                print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                    i + 1, self.n_iterations, loss.item(),\n",
    "                    self.model.covar_module.base_kernel.lengthscale.mean().item(),\n",
    "                    self.model.likelihood.second_noise_covar.noise.mean().item()\n",
    "                ))\n",
    "             \n",
    "            self.optimizer.step() \n",
    "    def predict(self, input): \n",
    "        \"\"\"\n",
    "        Make predictions using the GP model.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): The input data for making predictions.\n",
    "        \n",
    "        Returns:\n",
    "            dist (gpytorch.distributions.MultivariateNormal): The distribution representing the GP posterior.\n",
    "            observed_pred (gpytorch.distributions.MultivariateNormal): The predicted distribution considering the likelihood.\n",
    "            pred_means (torch.Tensor): The means of the predicted distributions.\n",
    "            class_pred (torch.Tensor): The predicted class labels.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "            dist = self.model(input)     # output distribution\n",
    "            pred_means = dist.loc          # means of distributino \n",
    "            observed_pred = self.likelihood(self.model(input))    # likelihood predictions mean and var  \n",
    "\n",
    "            class_pred = self.model(input).loc.max(0)[1]\n",
    "            \n",
    "        return dist, observed_pred, pred_means, class_pred\n",
    "    \n",
    "\n",
    "    def evaluate(self, x_input, y_true): \n",
    "        \"\"\"\n",
    "        Evaluate the GP model.\n",
    "\n",
    "        Args:\n",
    "            x_input (torch.Tensor): The input data features.\n",
    "            y_true (torch.Tensor): The true labels for the input data.\n",
    "        \n",
    "        Returns:\n",
    "            y_pred (numpy.ndarray): The predicted class labels.\n",
    "        \"\"\"\n",
    "        y_pred = self.model(x_input).loc.max(0)[1].numpy()\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def gp_results(self, x_input, y_true, plot_title=None): \n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics and print results.\n",
    "\n",
    "        Args:\n",
    "            x_input (torch.Tensor): The input data features.\n",
    "            y_true (torch.Tensor or numpy.ndarray): The true labels for the input data.\n",
    "            plot_title (str, optional): The title for the confusion matrix plot.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary containing evaluation metrics and confusion matrix components.\n",
    "        \"\"\"\n",
    "        y_pred = self.evaluate(x_input, y_true) \n",
    "        if isinstance(y_true, torch.Tensor):\n",
    "            y_true = y_true.numpy().reshape(-1)\n",
    "        # plot_confusion_matrix(y_true, y_pred, ['0','1'], title=plot_title)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        dist = self.model(x_input)     # get predicted distributions \n",
    "        pred_means = dist.loc          # means for predicted dist  \n",
    "\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        tp, tn, fp, fn = calculate_metrics(y_true, y_pred) \n",
    "        sensitivity = tp / (tp + fn) \n",
    "        specificity = tn / (tn + fp) \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print(f'accuracy: {accuracy:.4f}, precision: {precision:.4f}, recall: {recall:.4f}, specificity: {specificity:.4f}, cm: {cm}')\n",
    "        return {'accuracy': accuracy, 'precision': precision,  'recall':recall, 'specificity':specificity, 'TN': tn, 'FN': fn, 'FP': fp, 'TP': tp }\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03196461-714f-41ad-ab58-67c40014acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_torch_tens_float_mod(filepath, filename): \n",
    "    df = pd.read_csv(filepath+filename+'.csv')\n",
    "    traindf = df[df['subset'] == 'train'] \n",
    "    testdf = df[df['subset'] == 'test'] \n",
    "    cols = ['subset', 'compound_id', 'base_rdkit_smiles', 'fold','active']\n",
    "    \n",
    "    trainX_df = traindf.drop(columns=cols)\n",
    "    print(len(list(trainX_df.columns)))\n",
    "    # display(trainX_df)\n",
    "\n",
    "    trainy_df = traindf['active'] \n",
    "    testX_df = testdf.drop(columns=cols)\n",
    "    testy_df = testdf['active'] \n",
    "\n",
    "\n",
    "    train_x_temp = trainX_df.to_numpy().astype(\"double\") # double \n",
    "    test_x_temp = testX_df.to_numpy().astype(\"double\") #double \n",
    "    \n",
    "    train_y_temp = trainy_df.to_numpy().flatten().astype(\"double\") #double \n",
    "    test_y_temp = testy_df.to_numpy().flatten().astype(\"double\") #double \n",
    "    trainX = torch.as_tensor(train_x_temp, dtype=torch.float32)\n",
    "    trainy = torch.as_tensor(train_y_temp, dtype=torch.float32)\n",
    "    testX = torch.as_tensor(test_x_temp, dtype=torch.float32)\n",
    "    testy = torch.as_tensor(test_y_temp, dtype=torch.float32)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6faad9bf-cb3a-4e74-87d2-f8fe148378aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GP_path = '/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/undersampler_validation/under_batch2/'\n",
    "def save_results(trainX, trainy, testX, testy, root_name,GP_path, n_iterations=300, n_samples=100):\n",
    "    \"\"\"\n",
    "    Train a Dirichlet Gaussian Process model and save the training and test performance results.\n",
    "\n",
    "    This function trains a Dirichlet GP model on the given training data, evaluates it on both the training\n",
    "    and test data, and saves various performance metrics and predictions to pandas DataFrames.\n",
    "\n",
    "    Args:\n",
    "        trainX (torch.Tensor): The training data features.\n",
    "        trainy (torch.Tensor): The training data labels.\n",
    "        testX (torch.Tensor): The test data features.\n",
    "        testy (torch.Tensor): The test data labels.\n",
    "        root_name (str): The root name used for labeling the model in the results.\n",
    "        n_iterations (int, optional): The number of training iterations. Default is 300.\n",
    "        n_samples (int, optional): The number of samples for prediction. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "        train_perf_df (pd.DataFrame): DataFrame containing performance metrics and predictions for the training data.\n",
    "        test_perf_df (pd.DataFrame): DataFrame containing performance metrics and predictions for the test data.\n",
    "    \"\"\"\n",
    "    likelihood = DirichletClassificationLikelihood(trainy.long(), learn_additional_noise=True)\n",
    "    model = DirichletGPModel(trainX, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)\n",
    "    # n_iterations = 300\n",
    "    trainer = Trainer(model, likelihood, n_iterations)\n",
    "    trainer.train(trainX, trainy) \n",
    "  \n",
    "    train_dist, train_observed_pred, train_pred_means, train_pred  = trainer.predict(trainX)\n",
    "    train_results = trainer.gp_results(trainX, trainy)\n",
    "    test_dist, test_observed_pred, test_pred_means, test_pred  = trainer.predict(testX)\n",
    "    test_results = trainer.gp_results(testX, testy)\n",
    "    \n",
    "    train_observed_pred.mean.numpy()\n",
    "    train_pred_variance2D = train_observed_pred.variance.numpy()\n",
    "    test_observed_pred.mean.numpy()\n",
    "    test_pred_variance2D=test_observed_pred.variance.numpy()\n",
    "    \n",
    "    train_pred_samples = train_dist.sample(torch.Size((256,))).exp()\n",
    "    train_probabilities = (train_pred_samples / train_pred_samples.sum(-2, keepdim=True)).mean(0)\n",
    "\n",
    "    train_prob_stds = (train_pred_samples / train_pred_samples.sum(-2, keepdim=True)).std(0)\n",
    "\n",
    "    test_pred_samples = test_dist.sample(torch.Size((100,))).exp()\n",
    "\n",
    "    test_probabilities = (test_pred_samples / test_pred_samples.sum(-2, keepdim=True)).mean(0)\n",
    "    test_prob_stds = (test_pred_samples / test_pred_samples.sum(-2, keepdim=True)).std(0)\n",
    "\n",
    " \n",
    "    train_perf_df = pd.DataFrame()\n",
    "    test_perf_df = pd.DataFrame()\n",
    "    train_perf_df['mean_pred_class0'] = train_observed_pred.mean.numpy()[0,]\n",
    "    train_perf_df['mean_pred_class1'] = train_observed_pred.mean.numpy()[1,]\n",
    "    train_perf_df['y'] = trainy\n",
    "    train_perf_df['y_pred'] = train_pred_means.max(0)[1]\n",
    "    train_perf_df['var_pred_class0']=train_observed_pred.variance.numpy()[0,]\n",
    "    train_perf_df['var_pred_class1']=train_observed_pred.variance.numpy()[1,]\n",
    "    train_perf_df['pred_prob_class0'] = train_probabilities.numpy()[0,]\n",
    "    train_perf_df['pred_prob_class1'] = train_probabilities.numpy()[1,]\n",
    "    train_perf_df['pred_prob_std_class0'] = train_prob_stds.numpy()[0,]\n",
    "    train_perf_df['pred_prob_std_class1'] = train_prob_stds.numpy()[1,]\n",
    "    train_perf_df['model'] = f'{root_name}_GP_Dirichlet_matern'\n",
    "    train_perf_df['subset'] = 'train' \n",
    "    train_cm = confusion_matrix(trainy, train_perf_df['y_pred'])\n",
    "    cm_flattened = train_cm.flatten().tolist()\n",
    "    train_perf_df['cm']= [cm_flattened]* len(train_perf_df)\n",
    "    train_perf_df['prediction_type'] = train_perf_df.apply(lambda x: prediction_type(x['y'], x['y_pred']), axis=1)\n",
    "    train_perf_df['ROC-AUC'] = roc_auc_score(trainy, train_perf_df['y_pred'])\n",
    "    train_perf_df['MCC'] = matthews_corrcoef(trainy, train_perf_df['y_pred'])\n",
    "    train_perf_df['Balanced Accuracy'] = balanced_accuracy_score(trainy, train_perf_df['y_pred'])\n",
    "    train_perf_df['f1'] = f1_score(trainy, train_perf_df['y_pred'])\n",
    "    \n",
    "    test_perf_df['mean_pred_class0'] = test_observed_pred.mean.numpy()[0,]\n",
    "    test_perf_df['mean_pred_class1'] = test_observed_pred.mean.numpy()[1,]\n",
    "    test_perf_df['y'] = testy\n",
    "    test_perf_df['y_pred'] = test_pred_means.max(0)[1]\n",
    "    test_perf_df['var_pred_class0']=test_observed_pred.variance.numpy()[0,]\n",
    "    test_perf_df['var_pred_class1']=test_observed_pred.variance.numpy()[1,]\n",
    "    test_perf_df['pred_prob_class0'] = test_probabilities.numpy()[0,]\n",
    "    test_perf_df['pred_prob_class1'] = test_probabilities.numpy()[1,]\n",
    "    test_perf_df['pred_prob_std_class0'] =test_prob_stds.numpy()[0,]\n",
    "    test_perf_df['pred_prob_std_class1'] = test_prob_stds.numpy()[1,]\n",
    "    test_perf_df['model'] = f'{root_name}_GP_Dirichlet_matern'\n",
    "    test_perf_df['subset'] = 'test' \n",
    "    test_cm = confusion_matrix(testy, test_perf_df['y_pred'])\n",
    "    test_cm_flattened = test_cm.flatten().tolist()\n",
    "    test_perf_df['cm']= [test_cm_flattened]* len(test_perf_df)\n",
    "    test_perf_df['prediction_type'] = test_perf_df.apply(lambda x: prediction_type(x['y'], x['y_pred']), axis=1)\n",
    "    test_perf_df['ROC-AUC'] = roc_auc_score(testy, test_perf_df['y_pred'])\n",
    "    test_perf_df['MCC'] = matthews_corrcoef(testy, test_perf_df['y_pred'])\n",
    "    test_perf_df['Balanced Accuracy'] = balanced_accuracy_score(testy, test_perf_df['y_pred'])\n",
    "    test_perf_df['f1'] = f1_score(testy, test_perf_df['y_pred'])\n",
    "    with open(f'{GP_path}{root_name}_GP_Dirichlet_matern_model.pkl', 'wb') as f: \n",
    "        pickle.dump(model,f)\n",
    "    with open(f'{GP_path}{root_name}_GP_Dirichlet_matern_likelihood.pkl', 'wb') as f: \n",
    "        pickle.dump(likelihood,f)\n",
    "    for k, val in train_results.items(): \n",
    "        train_perf_df[k] = val\n",
    "    for k, val in test_results.items():\n",
    "        test_perf_df[k] = val\n",
    "    return train_perf_df, test_perf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb7b782a-0791-480d-8a34-cc415cb653eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEK2\n",
      "NEK2_binding_moe_UNDER_batch2\n",
      "306\n",
      "trainX:torch.Size([90, 306]), train y: torch.Size([90]), testX: torch.Size([283, 306]), test y: torch.Size([283])\n",
      "Iter 1/300 - Loss: 7.107   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 5.931   lengthscale: 1.112   noise: 1.297\n",
      "Iter 21/300 - Loss: 5.438   lengthscale: 1.703   noise: 1.982\n",
      "Iter 31/300 - Loss: 5.262   lengthscale: 2.701   noise: 2.572\n",
      "Iter 41/300 - Loss: 5.206   lengthscale: 3.933   noise: 3.007\n",
      "Iter 51/300 - Loss: 5.188   lengthscale: 5.093   noise: 3.302\n",
      "Iter 61/300 - Loss: 5.181   lengthscale: 5.985   noise: 3.484\n",
      "Iter 71/300 - Loss: 5.178   lengthscale: 6.620   noise: 3.582\n",
      "Iter 81/300 - Loss: 5.175   lengthscale: 7.082   noise: 3.622\n",
      "Iter 91/300 - Loss: 5.174   lengthscale: 7.441   noise: 3.624\n",
      "Iter 101/300 - Loss: 5.173   lengthscale: 7.739   noise: 3.600\n",
      "Iter 111/300 - Loss: 5.172   lengthscale: 7.997   noise: 3.560\n",
      "Iter 121/300 - Loss: 5.171   lengthscale: 8.227   noise: 3.510\n",
      "Iter 131/300 - Loss: 5.170   lengthscale: 8.434   noise: 3.453\n",
      "Iter 141/300 - Loss: 5.169   lengthscale: 8.620   noise: 3.391\n",
      "Iter 151/300 - Loss: 5.169   lengthscale: 8.790   noise: 3.326\n",
      "Iter 161/300 - Loss: 5.168   lengthscale: 8.943   noise: 3.257\n",
      "Iter 171/300 - Loss: 5.167   lengthscale: 9.083   noise: 3.186\n",
      "Iter 181/300 - Loss: 5.166   lengthscale: 9.210   noise: 3.112\n",
      "Iter 191/300 - Loss: 5.166   lengthscale: 9.326   noise: 3.035\n",
      "Iter 201/300 - Loss: 5.165   lengthscale: 9.432   noise: 2.956\n",
      "Iter 211/300 - Loss: 5.164   lengthscale: 9.528   noise: 2.875\n",
      "Iter 221/300 - Loss: 5.163   lengthscale: 9.616   noise: 2.791\n",
      "Iter 231/300 - Loss: 5.163   lengthscale: 9.696   noise: 2.706\n",
      "Iter 241/300 - Loss: 5.162   lengthscale: 9.768   noise: 2.619\n",
      "Iter 251/300 - Loss: 5.161   lengthscale: 9.834   noise: 2.530\n",
      "Iter 261/300 - Loss: 5.161   lengthscale: 9.894   noise: 2.441\n",
      "Iter 271/300 - Loss: 5.160   lengthscale: 9.949   noise: 2.350\n",
      "Iter 281/300 - Loss: 5.159   lengthscale: 9.999   noise: 2.259\n",
      "Iter 291/300 - Loss: 5.159   lengthscale: 10.045   noise: 2.168\n",
      "accuracy: 1.0000, precision: 1.0000, recall: 1.0000, specificity: 1.0000, cm: [[45  0]\n",
      " [ 0 45]]\n",
      "accuracy: 0.7703, precision: 0.1045, recall: 0.5833, specificity: 0.7786, cm: [[211  60]\n",
      " [  5   7]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEK2_binding_mfp_UNDER_batch2\n",
      "2048\n",
      "trainX:torch.Size([90, 2048]), train y: torch.Size([90]), testX: torch.Size([283, 2048]), test y: torch.Size([283])\n",
      "Iter 1/300 - Loss: 7.114   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 6.265   lengthscale: 0.443   noise: 1.318\n",
      "Iter 21/300 - Loss: 5.732   lengthscale: 0.225   noise: 2.137\n",
      "Iter 31/300 - Loss: 5.472   lengthscale: 0.107   noise: 2.987\n",
      "Iter 41/300 - Loss: 5.351   lengthscale: 0.054   noise: 3.752\n",
      "Iter 51/300 - Loss: 5.294   lengthscale: 0.027   noise: 4.391\n",
      "Iter 61/300 - Loss: 5.263   lengthscale: 0.014   noise: 4.901\n",
      "Iter 71/300 - Loss: 5.241   lengthscale: 0.009   noise: 5.302\n",
      "Iter 81/300 - Loss: 5.232   lengthscale: 0.006   noise: 5.623\n",
      "Iter 91/300 - Loss: 5.224   lengthscale: 0.005   noise: 5.888\n",
      "Iter 101/300 - Loss: 5.219   lengthscale: 0.004   noise: 6.114\n",
      "Iter 111/300 - Loss: 5.215   lengthscale: 0.003   noise: 6.311\n",
      "Iter 121/300 - Loss: 5.211   lengthscale: 0.003   noise: 6.485\n",
      "Iter 131/300 - Loss: 5.208   lengthscale: 0.003   noise: 6.639\n",
      "Iter 141/300 - Loss: 5.208   lengthscale: 0.003   noise: 6.775\n",
      "Iter 151/300 - Loss: 5.208   lengthscale: 0.003   noise: 6.896\n",
      "Iter 161/300 - Loss: 5.207   lengthscale: 0.003   noise: 7.003\n",
      "Iter 171/300 - Loss: 5.205   lengthscale: 0.002   noise: 7.098\n",
      "Iter 181/300 - Loss: 5.206   lengthscale: 0.002   noise: 7.182\n",
      "Iter 191/300 - Loss: 5.205   lengthscale: 0.002   noise: 7.258\n",
      "Iter 201/300 - Loss: 5.204   lengthscale: 0.002   noise: 7.325\n",
      "Iter 211/300 - Loss: 5.204   lengthscale: 0.002   noise: 7.384\n",
      "Iter 221/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.436\n",
      "Iter 231/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.482\n",
      "Iter 241/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.523\n",
      "Iter 251/300 - Loss: 5.202   lengthscale: 0.002   noise: 7.559\n",
      "Iter 261/300 - Loss: 5.202   lengthscale: 0.002   noise: 7.591\n",
      "Iter 271/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.618\n",
      "Iter 281/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.642\n",
      "Iter 291/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5000, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[45  0]\n",
      " [45  0]]\n",
      "accuracy: 0.9576, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[271   0]\n",
      " [ 12   0]]\n",
      "\n",
      "NEK2_inhibition_moe_UNDER_batch2\n",
      "306\n",
      "trainX:torch.Size([224, 306]), train y: torch.Size([224]), testX: torch.Size([408, 306]), test y: torch.Size([408])\n",
      "Iter 1/300 - Loss: 7.107   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 5.930   lengthscale: 1.290   noise: 1.297\n",
      "Iter 21/300 - Loss: 5.430   lengthscale: 2.091   noise: 1.981\n",
      "Iter 31/300 - Loss: 5.214   lengthscale: 3.189   noise: 2.555\n",
      "Iter 41/300 - Loss: 5.086   lengthscale: 4.507   noise: 2.904\n",
      "Iter 51/300 - Loss: 5.001   lengthscale: 5.821   noise: 2.984\n",
      "Iter 61/300 - Loss: 4.946   lengthscale: 6.963   noise: 2.831\n",
      "Iter 71/300 - Loss: 4.905   lengthscale: 7.904   noise: 2.523\n",
      "Iter 81/300 - Loss: 4.871   lengthscale: 8.683   noise: 2.133\n",
      "Iter 91/300 - Loss: 4.842   lengthscale: 9.349   noise: 1.722\n",
      "Iter 101/300 - Loss: 4.816   lengthscale: 9.935   noise: 1.337\n",
      "Iter 111/300 - Loss: 4.795   lengthscale: 10.462   noise: 1.011\n",
      "Iter 121/300 - Loss: 4.778   lengthscale: 10.945   noise: 0.758\n",
      "Iter 131/300 - Loss: 4.765   lengthscale: 11.392   noise: 0.572\n",
      "Iter 141/300 - Loss: 4.754   lengthscale: 11.809   noise: 0.440\n",
      "Iter 151/300 - Loss: 4.745   lengthscale: 12.203   noise: 0.347\n",
      "Iter 161/300 - Loss: 4.738   lengthscale: 12.578   noise: 0.280\n",
      "Iter 171/300 - Loss: 4.732   lengthscale: 12.937   noise: 0.231\n",
      "Iter 181/300 - Loss: 4.728   lengthscale: 13.282   noise: 0.195\n",
      "Iter 191/300 - Loss: 4.724   lengthscale: 13.615   noise: 0.167\n",
      "Iter 201/300 - Loss: 4.720   lengthscale: 13.937   noise: 0.146\n",
      "Iter 211/300 - Loss: 4.717   lengthscale: 14.249   noise: 0.128\n",
      "Iter 221/300 - Loss: 4.714   lengthscale: 14.551   noise: 0.114\n",
      "Iter 231/300 - Loss: 4.711   lengthscale: 14.845   noise: 0.103\n",
      "Iter 241/300 - Loss: 4.709   lengthscale: 15.130   noise: 0.093\n",
      "Iter 251/300 - Loss: 4.707   lengthscale: 15.408   noise: 0.085\n",
      "Iter 261/300 - Loss: 4.705   lengthscale: 15.679   noise: 0.078\n",
      "Iter 271/300 - Loss: 4.703   lengthscale: 15.944   noise: 0.071\n",
      "Iter 281/300 - Loss: 4.701   lengthscale: 16.202   noise: 0.066\n",
      "Iter 291/300 - Loss: 4.700   lengthscale: 16.455   noise: 0.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0000, precision: 1.0000, recall: 1.0000, specificity: 1.0000, cm: [[112   0]\n",
      " [  0 112]]\n",
      "accuracy: 0.8431, precision: 0.2692, recall: 0.7500, specificity: 0.8500, cm: [[323  57]\n",
      " [  7  21]]\n",
      "\n",
      "NEK2_inhibition_mfp_UNDER_batch2\n",
      "2048\n",
      "trainX:torch.Size([224, 2048]), train y: torch.Size([224]), testX: torch.Size([408, 2048]), test y: torch.Size([408])\n",
      "Iter 1/300 - Loss: 7.111   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 6.250   lengthscale: 1.276   noise: 1.319\n",
      "Iter 21/300 - Loss: 5.729   lengthscale: 2.123   noise: 2.143\n",
      "Iter 31/300 - Loss: 5.473   lengthscale: 2.863   noise: 3.003\n",
      "Iter 41/300 - Loss: 5.355   lengthscale: 3.249   noise: 3.762\n",
      "Iter 51/300 - Loss: 5.297   lengthscale: 3.414   noise: 4.382\n",
      "Iter 61/300 - Loss: 5.266   lengthscale: 3.482   noise: 4.878\n",
      "Iter 71/300 - Loss: 5.247   lengthscale: 3.509   noise: 5.276\n",
      "Iter 81/300 - Loss: 5.235   lengthscale: 3.521   noise: 5.600\n",
      "Iter 91/300 - Loss: 5.227   lengthscale: 3.526   noise: 5.871\n",
      "Iter 101/300 - Loss: 5.221   lengthscale: 3.529   noise: 6.101\n",
      "Iter 111/300 - Loss: 5.217   lengthscale: 3.531   noise: 6.301\n",
      "Iter 121/300 - Loss: 5.213   lengthscale: 3.532   noise: 6.476\n",
      "Iter 131/300 - Loss: 5.211   lengthscale: 3.532   noise: 6.631\n",
      "Iter 141/300 - Loss: 5.209   lengthscale: 3.533   noise: 6.769\n",
      "Iter 151/300 - Loss: 5.208   lengthscale: 3.533   noise: 6.891\n",
      "Iter 161/300 - Loss: 5.207   lengthscale: 3.534   noise: 6.999\n",
      "Iter 171/300 - Loss: 5.206   lengthscale: 3.534   noise: 7.095\n",
      "Iter 181/300 - Loss: 5.205   lengthscale: 3.534   noise: 7.181\n",
      "Iter 191/300 - Loss: 5.205   lengthscale: 3.535   noise: 7.257\n",
      "Iter 201/300 - Loss: 5.204   lengthscale: 3.535   noise: 7.325\n",
      "Iter 211/300 - Loss: 5.204   lengthscale: 3.535   noise: 7.385\n",
      "Iter 221/300 - Loss: 5.204   lengthscale: 3.535   noise: 7.438\n",
      "Iter 231/300 - Loss: 5.203   lengthscale: 3.535   noise: 7.485\n",
      "Iter 241/300 - Loss: 5.203   lengthscale: 3.536   noise: 7.526\n",
      "Iter 251/300 - Loss: 5.203   lengthscale: 3.536   noise: 7.562\n",
      "Iter 261/300 - Loss: 5.203   lengthscale: 3.536   noise: 7.594\n",
      "Iter 271/300 - Loss: 5.203   lengthscale: 3.536   noise: 7.622\n",
      "Iter 281/300 - Loss: 5.203   lengthscale: 3.536   noise: 7.647\n",
      "Iter 291/300 - Loss: 5.203   lengthscale: 3.536   noise: 7.668\n",
      "accuracy: 0.5000, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[112   0]\n",
      " [112   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9314, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[380   0]\n",
      " [ 28   0]]\n",
      "\n",
      "NEK3\n",
      "NEK3_binding_moe_UNDER_batch2\n",
      "306\n",
      "trainX:torch.Size([128, 306]), train y: torch.Size([128]), testX: torch.Size([282, 306]), test y: torch.Size([282])\n",
      "Iter 1/300 - Loss: 7.107   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 5.931   lengthscale: 1.247   noise: 1.297\n",
      "Iter 21/300 - Loss: 5.439   lengthscale: 2.029   noise: 1.982\n",
      "Iter 31/300 - Loss: 5.264   lengthscale: 3.122   noise: 2.574\n",
      "Iter 41/300 - Loss: 5.207   lengthscale: 4.412   noise: 3.011\n",
      "Iter 51/300 - Loss: 5.190   lengthscale: 5.545   noise: 3.307\n",
      "Iter 61/300 - Loss: 5.185   lengthscale: 6.317   noise: 3.494\n",
      "Iter 71/300 - Loss: 5.183   lengthscale: 6.792   noise: 3.601\n",
      "Iter 81/300 - Loss: 5.182   lengthscale: 7.087   noise: 3.655\n",
      "Iter 91/300 - Loss: 5.181   lengthscale: 7.286   noise: 3.672\n",
      "Iter 101/300 - Loss: 5.181   lengthscale: 7.435   noise: 3.664\n",
      "Iter 111/300 - Loss: 5.180   lengthscale: 7.556   noise: 3.641\n",
      "Iter 121/300 - Loss: 5.180   lengthscale: 7.658   noise: 3.609\n",
      "Iter 131/300 - Loss: 5.180   lengthscale: 7.746   noise: 3.570\n",
      "Iter 141/300 - Loss: 5.180   lengthscale: 7.821   noise: 3.527\n",
      "Iter 151/300 - Loss: 5.179   lengthscale: 7.886   noise: 3.481\n",
      "Iter 161/300 - Loss: 5.179   lengthscale: 7.941   noise: 3.433\n",
      "Iter 171/300 - Loss: 5.179   lengthscale: 7.988   noise: 3.384\n",
      "Iter 181/300 - Loss: 5.179   lengthscale: 8.029   noise: 3.332\n",
      "Iter 191/300 - Loss: 5.178   lengthscale: 8.064   noise: 3.280\n",
      "Iter 201/300 - Loss: 5.178   lengthscale: 8.095   noise: 3.225\n",
      "Iter 211/300 - Loss: 5.178   lengthscale: 8.122   noise: 3.170\n",
      "Iter 221/300 - Loss: 5.178   lengthscale: 8.146   noise: 3.112\n",
      "Iter 231/300 - Loss: 5.177   lengthscale: 8.167   noise: 3.054\n",
      "Iter 241/300 - Loss: 5.177   lengthscale: 8.185   noise: 2.994\n",
      "Iter 251/300 - Loss: 5.177   lengthscale: 8.202   noise: 2.933\n",
      "Iter 261/300 - Loss: 5.176   lengthscale: 8.216   noise: 2.872\n",
      "Iter 271/300 - Loss: 5.176   lengthscale: 8.230   noise: 2.809\n",
      "Iter 281/300 - Loss: 5.176   lengthscale: 8.241   noise: 2.745\n",
      "Iter 291/300 - Loss: 5.176   lengthscale: 8.252   noise: 2.681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0000, precision: 1.0000, recall: 1.0000, specificity: 1.0000, cm: [[64  0]\n",
      " [ 0 64]]\n",
      "accuracy: 0.6667, precision: 0.0947, recall: 0.5294, specificity: 0.6755, cm: [[179  86]\n",
      " [  8   9]]\n",
      "\n",
      "NEK3_binding_mfp_UNDER_batch2\n",
      "2048\n",
      "trainX:torch.Size([128, 2048]), train y: torch.Size([128]), testX: torch.Size([282, 2048]), test y: torch.Size([282])\n",
      "Iter 1/300 - Loss: 7.113   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 6.261   lengthscale: 1.193   noise: 1.319\n",
      "Iter 21/300 - Loss: 5.734   lengthscale: 2.024   noise: 2.139\n",
      "Iter 31/300 - Loss: 5.471   lengthscale: 2.855   noise: 2.996\n",
      "Iter 41/300 - Loss: 5.354   lengthscale: 3.295   noise: 3.759\n",
      "Iter 51/300 - Loss: 5.297   lengthscale: 3.480   noise: 4.390\n",
      "Iter 61/300 - Loss: 5.265   lengthscale: 3.555   noise: 4.895\n",
      "Iter 71/300 - Loss: 5.246   lengthscale: 3.586   noise: 5.297\n",
      "Iter 81/300 - Loss: 5.234   lengthscale: 3.599   noise: 5.621\n",
      "Iter 91/300 - Loss: 5.226   lengthscale: 3.605   noise: 5.889\n",
      "Iter 101/300 - Loss: 5.220   lengthscale: 3.608   noise: 6.117\n",
      "Iter 111/300 - Loss: 5.216   lengthscale: 3.610   noise: 6.315\n",
      "Iter 121/300 - Loss: 5.213   lengthscale: 3.611   noise: 6.490\n",
      "Iter 131/300 - Loss: 5.211   lengthscale: 3.612   noise: 6.644\n",
      "Iter 141/300 - Loss: 5.209   lengthscale: 3.612   noise: 6.781\n",
      "Iter 151/300 - Loss: 5.208   lengthscale: 3.613   noise: 6.902\n",
      "Iter 161/300 - Loss: 5.207   lengthscale: 3.613   noise: 7.009\n",
      "Iter 171/300 - Loss: 5.206   lengthscale: 3.613   noise: 7.105\n",
      "Iter 181/300 - Loss: 5.205   lengthscale: 3.613   noise: 7.190\n",
      "Iter 191/300 - Loss: 5.205   lengthscale: 3.613   noise: 7.265\n",
      "Iter 201/300 - Loss: 5.204   lengthscale: 3.613   noise: 7.332\n",
      "Iter 211/300 - Loss: 5.204   lengthscale: 3.613   noise: 7.392\n",
      "Iter 221/300 - Loss: 5.204   lengthscale: 3.613   noise: 7.444\n",
      "Iter 231/300 - Loss: 5.203   lengthscale: 3.613   noise: 7.491\n",
      "Iter 241/300 - Loss: 5.203   lengthscale: 3.613   noise: 7.531\n",
      "Iter 251/300 - Loss: 5.203   lengthscale: 3.613   noise: 7.567\n",
      "Iter 261/300 - Loss: 5.203   lengthscale: 3.613   noise: 7.599\n",
      "Iter 271/300 - Loss: 5.203   lengthscale: 3.613   noise: 7.626\n",
      "Iter 281/300 - Loss: 5.203   lengthscale: 3.613   noise: 7.650\n",
      "Iter 291/300 - Loss: 5.203   lengthscale: 3.613   noise: 7.671\n",
      "accuracy: 0.5000, precision: 0.5000, recall: 1.0000, specificity: 0.0000, cm: [[ 0 64]\n",
      " [ 0 64]]\n",
      "accuracy: 0.0603, precision: 0.0603, recall: 1.0000, specificity: 0.0000, cm: [[  0 265]\n",
      " [  0  17]]\n",
      "\n",
      "NEK5\n",
      "NEK5_binding_moe_UNDER_batch2\n",
      "306\n",
      "trainX:torch.Size([154, 306]), train y: torch.Size([154]), testX: torch.Size([248, 306]), test y: torch.Size([248])\n",
      "Iter 1/300 - Loss: 7.107   lengthscale: 0.693   noise: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11/300 - Loss: 5.931   lengthscale: 1.281   noise: 1.297\n",
      "Iter 21/300 - Loss: 5.436   lengthscale: 2.112   noise: 1.981\n",
      "Iter 31/300 - Loss: 5.251   lengthscale: 3.228   noise: 2.568\n",
      "Iter 41/300 - Loss: 5.176   lengthscale: 4.549   noise: 2.981\n",
      "Iter 51/300 - Loss: 5.140   lengthscale: 5.854   noise: 3.219\n",
      "Iter 61/300 - Loss: 5.121   lengthscale: 6.981   noise: 3.310\n",
      "Iter 71/300 - Loss: 5.109   lengthscale: 7.899   noise: 3.296\n",
      "Iter 81/300 - Loss: 5.101   lengthscale: 8.646   noise: 3.214\n",
      "Iter 91/300 - Loss: 5.094   lengthscale: 9.266   noise: 3.092\n",
      "Iter 101/300 - Loss: 5.088   lengthscale: 9.795   noise: 2.948\n",
      "Iter 111/300 - Loss: 5.084   lengthscale: 10.257   noise: 2.792\n",
      "Iter 121/300 - Loss: 5.079   lengthscale: 10.666   noise: 2.632\n",
      "Iter 131/300 - Loss: 5.075   lengthscale: 11.033   noise: 2.472\n",
      "Iter 141/300 - Loss: 5.071   lengthscale: 11.365   noise: 2.313\n",
      "Iter 151/300 - Loss: 5.067   lengthscale: 11.667   noise: 2.157\n",
      "Iter 161/300 - Loss: 5.064   lengthscale: 11.943   noise: 2.005\n",
      "Iter 171/300 - Loss: 5.061   lengthscale: 12.197   noise: 1.856\n",
      "Iter 181/300 - Loss: 5.058   lengthscale: 12.431   noise: 1.711\n",
      "Iter 191/300 - Loss: 5.055   lengthscale: 12.647   noise: 1.572\n",
      "Iter 201/300 - Loss: 5.052   lengthscale: 12.849   noise: 1.438\n",
      "Iter 211/300 - Loss: 5.049   lengthscale: 13.037   noise: 1.310\n",
      "Iter 221/300 - Loss: 5.047   lengthscale: 13.213   noise: 1.189\n",
      "Iter 231/300 - Loss: 5.045   lengthscale: 13.379   noise: 1.075\n",
      "Iter 241/300 - Loss: 5.043   lengthscale: 13.537   noise: 0.970\n",
      "Iter 251/300 - Loss: 5.041   lengthscale: 13.689   noise: 0.872\n",
      "Iter 261/300 - Loss: 5.039   lengthscale: 13.834   noise: 0.782\n",
      "Iter 271/300 - Loss: 5.038   lengthscale: 13.976   noise: 0.701\n",
      "Iter 281/300 - Loss: 5.036   lengthscale: 14.115   noise: 0.628\n",
      "Iter 291/300 - Loss: 5.035   lengthscale: 14.252   noise: 0.562\n",
      "accuracy: 1.0000, precision: 1.0000, recall: 1.0000, specificity: 1.0000, cm: [[77  0]\n",
      " [ 0 77]]\n",
      "accuracy: 0.7218, precision: 0.1975, recall: 0.8000, specificity: 0.7149, cm: [[163  65]\n",
      " [  4  16]]\n",
      "\n",
      "NEK5_binding_mfp_UNDER_batch2\n",
      "2048\n",
      "trainX:torch.Size([154, 2048]), train y: torch.Size([154]), testX: torch.Size([248, 2048]), test y: torch.Size([248])\n",
      "Iter 1/300 - Loss: 7.111   lengthscale: 0.693   noise: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 11/300 - Loss: 6.257   lengthscale: 1.259   noise: 1.319\n",
      "Iter 21/300 - Loss: 5.732   lengthscale: 2.110   noise: 2.140\n",
      "Iter 31/300 - Loss: 5.471   lengthscale: 2.898   noise: 2.998\n",
      "Iter 41/300 - Loss: 5.354   lengthscale: 3.306   noise: 3.759\n",
      "Iter 51/300 - Loss: 5.297   lengthscale: 3.478   noise: 4.386\n",
      "Iter 61/300 - Loss: 5.265   lengthscale: 3.548   noise: 4.889\n",
      "Iter 71/300 - Loss: 5.246   lengthscale: 3.576   noise: 5.289\n",
      "Iter 81/300 - Loss: 5.234   lengthscale: 3.588   noise: 5.614\n",
      "Iter 91/300 - Loss: 5.226   lengthscale: 3.593   noise: 5.883\n",
      "Iter 101/300 - Loss: 5.221   lengthscale: 3.596   noise: 6.112\n",
      "Iter 111/300 - Loss: 5.216   lengthscale: 3.598   noise: 6.311\n",
      "Iter 121/300 - Loss: 5.213   lengthscale: 3.599   noise: 6.485\n",
      "Iter 131/300 - Loss: 5.211   lengthscale: 3.600   noise: 6.640\n",
      "Iter 141/300 - Loss: 5.209   lengthscale: 3.600   noise: 6.777\n",
      "Iter 151/300 - Loss: 5.208   lengthscale: 3.601   noise: 6.898\n",
      "Iter 161/300 - Loss: 5.207   lengthscale: 3.601   noise: 7.006\n",
      "Iter 171/300 - Loss: 5.206   lengthscale: 3.601   noise: 7.102\n",
      "Iter 181/300 - Loss: 5.205   lengthscale: 3.602   noise: 7.187\n",
      "Iter 191/300 - Loss: 5.205   lengthscale: 3.602   noise: 7.263\n",
      "Iter 201/300 - Loss: 5.204   lengthscale: 3.602   noise: 7.330\n",
      "Iter 211/300 - Loss: 5.204   lengthscale: 3.602   noise: 7.390\n",
      "Iter 221/300 - Loss: 5.204   lengthscale: 3.602   noise: 7.442\n",
      "Iter 231/300 - Loss: 5.203   lengthscale: 3.602   noise: 7.489\n",
      "Iter 241/300 - Loss: 5.203   lengthscale: 3.602   noise: 7.530\n",
      "Iter 251/300 - Loss: 5.203   lengthscale: 3.602   noise: 7.566\n",
      "Iter 261/300 - Loss: 5.203   lengthscale: 3.603   noise: 7.597\n",
      "Iter 271/300 - Loss: 5.203   lengthscale: 3.603   noise: 7.625\n",
      "Iter 281/300 - Loss: 5.203   lengthscale: 3.603   noise: 7.649\n",
      "Iter 291/300 - Loss: 5.203   lengthscale: 3.603   noise: 7.670\n",
      "accuracy: 0.5000, precision: 0.5000, recall: 1.0000, specificity: 0.0000, cm: [[ 0 77]\n",
      " [ 0 77]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0806, precision: 0.0806, recall: 1.0000, specificity: 0.0000, cm: [[  0 228]\n",
      " [  0  20]]\n",
      "\n",
      "NEK9\n",
      "NEK9_binding_moe_UNDER_batch2\n",
      "306\n",
      "trainX:torch.Size([96, 306]), train y: torch.Size([96]), testX: torch.Size([283, 306]), test y: torch.Size([283])\n",
      "Iter 1/300 - Loss: 7.107   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 5.931   lengthscale: 1.247   noise: 1.297\n",
      "Iter 21/300 - Loss: 5.438   lengthscale: 2.031   noise: 1.982\n",
      "Iter 31/300 - Loss: 5.260   lengthscale: 3.128   noise: 2.572\n",
      "Iter 41/300 - Loss: 5.195   lengthscale: 4.450   noise: 3.000\n",
      "Iter 51/300 - Loss: 5.164   lengthscale: 5.813   noise: 3.269\n",
      "Iter 61/300 - Loss: 5.147   lengthscale: 7.054   noise: 3.404\n",
      "Iter 71/300 - Loss: 5.137   lengthscale: 8.106   noise: 3.437\n",
      "Iter 81/300 - Loss: 5.130   lengthscale: 8.975   noise: 3.403\n",
      "Iter 91/300 - Loss: 5.124   lengthscale: 9.698   noise: 3.327\n",
      "Iter 101/300 - Loss: 5.120   lengthscale: 10.308   noise: 3.229\n",
      "Iter 111/300 - Loss: 5.117   lengthscale: 10.833   noise: 3.119\n",
      "Iter 121/300 - Loss: 5.114   lengthscale: 11.292   noise: 3.005\n",
      "Iter 131/300 - Loss: 5.111   lengthscale: 11.697   noise: 2.891\n",
      "Iter 141/300 - Loss: 5.109   lengthscale: 12.058   noise: 2.777\n",
      "Iter 151/300 - Loss: 5.106   lengthscale: 12.380   noise: 2.666\n",
      "Iter 161/300 - Loss: 5.104   lengthscale: 12.671   noise: 2.556\n",
      "Iter 171/300 - Loss: 5.103   lengthscale: 12.933   noise: 2.449\n",
      "Iter 181/300 - Loss: 5.101   lengthscale: 13.169   noise: 2.345\n",
      "Iter 191/300 - Loss: 5.099   lengthscale: 13.383   noise: 2.242\n",
      "Iter 201/300 - Loss: 5.098   lengthscale: 13.577   noise: 2.143\n",
      "Iter 211/300 - Loss: 5.097   lengthscale: 13.752   noise: 2.045\n",
      "Iter 221/300 - Loss: 5.095   lengthscale: 13.910   noise: 1.950\n",
      "Iter 231/300 - Loss: 5.094   lengthscale: 14.052   noise: 1.858\n",
      "Iter 241/300 - Loss: 5.093   lengthscale: 14.181   noise: 1.768\n",
      "Iter 251/300 - Loss: 5.092   lengthscale: 14.296   noise: 1.680\n",
      "Iter 261/300 - Loss: 5.091   lengthscale: 14.399   noise: 1.596\n",
      "Iter 271/300 - Loss: 5.090   lengthscale: 14.492   noise: 1.513\n",
      "Iter 281/300 - Loss: 5.089   lengthscale: 14.574   noise: 1.434\n",
      "Iter 291/300 - Loss: 5.089   lengthscale: 14.648   noise: 1.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0000, precision: 1.0000, recall: 1.0000, specificity: 1.0000, cm: [[48  0]\n",
      " [ 0 48]]\n",
      "accuracy: 0.6678, precision: 0.0990, recall: 0.7692, specificity: 0.6630, cm: [[179  91]\n",
      " [  3  10]]\n",
      "\n",
      "NEK9_binding_mfp_UNDER_batch2\n",
      "2048\n",
      "trainX:torch.Size([96, 2048]), train y: torch.Size([96]), testX: torch.Size([283, 2048]), test y: torch.Size([283])\n",
      "Iter 1/300 - Loss: 7.114   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 6.258   lengthscale: 1.157   noise: 1.319\n",
      "Iter 21/300 - Loss: 5.736   lengthscale: 1.961   noise: 2.139\n",
      "Iter 31/300 - Loss: 5.471   lengthscale: 2.806   noise: 2.995\n",
      "Iter 41/300 - Loss: 5.354   lengthscale: 3.259   noise: 3.759\n",
      "Iter 51/300 - Loss: 5.297   lengthscale: 3.450   noise: 4.393\n",
      "Iter 61/300 - Loss: 5.265   lengthscale: 3.529   noise: 4.899\n",
      "Iter 71/300 - Loss: 5.246   lengthscale: 3.561   noise: 5.301\n",
      "Iter 81/300 - Loss: 5.234   lengthscale: 3.574   noise: 5.625\n",
      "Iter 91/300 - Loss: 5.226   lengthscale: 3.581   noise: 5.893\n",
      "Iter 101/300 - Loss: 5.220   lengthscale: 3.584   noise: 6.120\n",
      "Iter 111/300 - Loss: 5.216   lengthscale: 3.586   noise: 6.318\n",
      "Iter 121/300 - Loss: 5.213   lengthscale: 3.587   noise: 6.493\n",
      "Iter 131/300 - Loss: 5.211   lengthscale: 3.588   noise: 6.647\n",
      "Iter 141/300 - Loss: 5.209   lengthscale: 3.588   noise: 6.783\n",
      "Iter 151/300 - Loss: 5.208   lengthscale: 3.588   noise: 6.904\n",
      "Iter 161/300 - Loss: 5.207   lengthscale: 3.588   noise: 7.011\n",
      "Iter 171/300 - Loss: 5.206   lengthscale: 3.588   noise: 7.107\n",
      "Iter 181/300 - Loss: 5.205   lengthscale: 3.588   noise: 7.192\n",
      "Iter 191/300 - Loss: 5.205   lengthscale: 3.588   noise: 7.267\n",
      "Iter 201/300 - Loss: 5.204   lengthscale: 3.588   noise: 7.334\n",
      "Iter 211/300 - Loss: 5.204   lengthscale: 3.588   noise: 7.393\n",
      "Iter 221/300 - Loss: 5.204   lengthscale: 3.588   noise: 7.445\n",
      "Iter 231/300 - Loss: 5.203   lengthscale: 3.588   noise: 7.492\n",
      "Iter 241/300 - Loss: 5.203   lengthscale: 3.588   noise: 7.532\n",
      "Iter 251/300 - Loss: 5.203   lengthscale: 3.588   noise: 7.568\n",
      "Iter 261/300 - Loss: 5.203   lengthscale: 3.588   noise: 7.600\n",
      "Iter 271/300 - Loss: 5.203   lengthscale: 3.588   noise: 7.627\n",
      "Iter 281/300 - Loss: 5.203   lengthscale: 3.588   noise: 7.651\n",
      "Iter 291/300 - Loss: 5.203   lengthscale: 3.588   noise: 7.672\n",
      "accuracy: 0.5000, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[48  0]\n",
      " [48  0]]\n",
      "accuracy: 0.9541, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[270   0]\n",
      " [ 13   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NEK9_inhibition_moe_UNDER_batch2\n",
      "306\n",
      "trainX:torch.Size([66, 306]), train y: torch.Size([66]), testX: torch.Size([80, 306]), test y: torch.Size([80])\n",
      "Iter 1/300 - Loss: 7.107   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 5.931   lengthscale: 1.239   noise: 1.297\n",
      "Iter 21/300 - Loss: 5.436   lengthscale: 1.990   noise: 1.982\n",
      "Iter 31/300 - Loss: 5.242   lengthscale: 3.056   noise: 2.566\n",
      "Iter 41/300 - Loss: 5.141   lengthscale: 4.377   noise: 2.957\n",
      "Iter 51/300 - Loss: 5.077   lengthscale: 5.742   noise: 3.126\n",
      "Iter 61/300 - Loss: 5.031   lengthscale: 6.965   noise: 3.090\n",
      "Iter 71/300 - Loss: 4.994   lengthscale: 8.003   noise: 2.892\n",
      "Iter 81/300 - Loss: 4.961   lengthscale: 8.887   noise: 2.580\n",
      "Iter 91/300 - Loss: 4.930   lengthscale: 9.655   noise: 2.196\n",
      "Iter 101/300 - Loss: 4.900   lengthscale: 10.343   noise: 1.781\n",
      "Iter 111/300 - Loss: 4.872   lengthscale: 10.972   noise: 1.375\n",
      "Iter 121/300 - Loss: 4.847   lengthscale: 11.557   noise: 1.014\n",
      "Iter 131/300 - Loss: 4.824   lengthscale: 12.106   noise: 0.727\n",
      "Iter 141/300 - Loss: 4.806   lengthscale: 12.625   noise: 0.518\n",
      "Iter 151/300 - Loss: 4.792   lengthscale: 13.118   noise: 0.376\n",
      "Iter 161/300 - Loss: 4.781   lengthscale: 13.588   noise: 0.282\n",
      "Iter 171/300 - Loss: 4.773   lengthscale: 14.038   noise: 0.219\n",
      "Iter 181/300 - Loss: 4.766   lengthscale: 14.468   noise: 0.176\n",
      "Iter 191/300 - Loss: 4.760   lengthscale: 14.881   noise: 0.145\n",
      "Iter 201/300 - Loss: 4.755   lengthscale: 15.277   noise: 0.122\n",
      "Iter 211/300 - Loss: 4.751   lengthscale: 15.658   noise: 0.105\n",
      "Iter 221/300 - Loss: 4.747   lengthscale: 16.024   noise: 0.092\n",
      "Iter 231/300 - Loss: 4.744   lengthscale: 16.377   noise: 0.081\n",
      "Iter 241/300 - Loss: 4.742   lengthscale: 16.718   noise: 0.073\n",
      "Iter 251/300 - Loss: 4.739   lengthscale: 17.048   noise: 0.066\n",
      "Iter 261/300 - Loss: 4.737   lengthscale: 17.367   noise: 0.059\n",
      "Iter 271/300 - Loss: 4.735   lengthscale: 17.677   noise: 0.054\n",
      "Iter 281/300 - Loss: 4.733   lengthscale: 17.977   noise: 0.050\n",
      "Iter 291/300 - Loss: 4.731   lengthscale: 18.270   noise: 0.046\n",
      "accuracy: 1.0000, precision: 1.0000, recall: 1.0000, specificity: 1.0000, cm: [[33  0]\n",
      " [ 0 33]]\n",
      "accuracy: 0.8750, precision: 0.4667, recall: 0.7778, specificity: 0.8873, cm: [[63  8]\n",
      " [ 2  7]]\n",
      "\n",
      "NEK9_inhibition_mfp_UNDER_batch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "trainX:torch.Size([66, 2048]), train y: torch.Size([66]), testX: torch.Size([80, 2048]), test y: torch.Size([80])\n",
      "Iter 1/300 - Loss: 7.114   lengthscale: 0.693   noise: 0.693\n",
      "Iter 11/300 - Loss: 6.262   lengthscale: 0.641   noise: 1.319\n",
      "Iter 21/300 - Loss: 5.741   lengthscale: 0.367   noise: 2.138\n",
      "Iter 31/300 - Loss: 5.468   lengthscale: 0.188   noise: 2.989\n",
      "Iter 41/300 - Loss: 5.353   lengthscale: 0.089   noise: 3.755\n",
      "Iter 51/300 - Loss: 5.297   lengthscale: 0.047   noise: 4.394\n",
      "Iter 61/300 - Loss: 5.263   lengthscale: 0.028   noise: 4.905\n",
      "Iter 71/300 - Loss: 5.246   lengthscale: 0.018   noise: 5.308\n",
      "Iter 81/300 - Loss: 5.233   lengthscale: 0.012   noise: 5.630\n",
      "Iter 91/300 - Loss: 5.224   lengthscale: 0.008   noise: 5.895\n",
      "Iter 101/300 - Loss: 5.220   lengthscale: 0.005   noise: 6.121\n",
      "Iter 111/300 - Loss: 5.215   lengthscale: 0.004   noise: 6.317\n",
      "Iter 121/300 - Loss: 5.212   lengthscale: 0.003   noise: 6.489\n",
      "Iter 131/300 - Loss: 5.210   lengthscale: 0.003   noise: 6.642\n",
      "Iter 141/300 - Loss: 5.208   lengthscale: 0.002   noise: 6.777\n",
      "Iter 151/300 - Loss: 5.207   lengthscale: 0.002   noise: 6.897\n",
      "Iter 161/300 - Loss: 5.207   lengthscale: 0.002   noise: 7.003\n",
      "Iter 171/300 - Loss: 5.205   lengthscale: 0.002   noise: 7.098\n",
      "Iter 181/300 - Loss: 5.204   lengthscale: 0.002   noise: 7.182\n",
      "Iter 191/300 - Loss: 5.205   lengthscale: 0.002   noise: 7.257\n",
      "Iter 201/300 - Loss: 5.204   lengthscale: 0.002   noise: 7.324\n",
      "Iter 211/300 - Loss: 5.204   lengthscale: 0.002   noise: 7.383\n",
      "Iter 221/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.435\n",
      "Iter 231/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.482\n",
      "Iter 241/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.523\n",
      "Iter 251/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.559\n",
      "Iter 261/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.590\n",
      "Iter 271/300 - Loss: 5.203   lengthscale: 0.002   noise: 7.618\n",
      "Iter 281/300 - Loss: 5.202   lengthscale: 0.002   noise: 7.642\n",
      "Iter 291/300 - Loss: 5.203   lengthscale: 0.001   noise: 7.663\n",
      "accuracy: 0.5000, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[33  0]\n",
      " [33  0]]\n",
      "accuracy: 0.8875, precision: 0.0000, recall: 0.0000, specificity: 1.0000, cm: [[71  0]\n",
      " [ 9  0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/gpytorch/models/exact_gp.py:284: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/jayceepang/msse/capstone/gpytorch-venv/lib/python3.9/site-packages/linear_operator/operators/_linear_operator.py:2155: NumericalWarning: Runtime Error when computing Cholesky decomposition: Matrix not positive definite after repeatedly adding jitter up to 1.0e-04.. Using symeig method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "GP_path = '/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/undersampler_validation/under_batch2/'\n",
    "samplings = ['UNDER'] \n",
    "feat_types = ['moe', 'mfp']\n",
    "neks = ['2', '3', '5', '9']\n",
    "for nek in neks:\n",
    "    print(f'NEK{nek}')\n",
    "    \n",
    "    if nek in ['2','9']: \n",
    "        bind_inhib = ['binding', 'inhibition']\n",
    "    else: \n",
    "        bind_inhib = ['binding']\n",
    "    for bi in bind_inhib: \n",
    "        if bi == 'binding': \n",
    "            this_bi = 'bind' \n",
    "        if bi == 'inhibition': \n",
    "            this_bi = 'inhib'\n",
    "        for feat in feat_types: \n",
    "            for samp in samplings: \n",
    "                this_dir = f'/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/undersampler_validation/under_batch2/'\n",
    "                file_root = f'NEK{nek}_{bi}_{feat}_{samp}_batch2'\n",
    "                print(file_root)\n",
    "                trainX, trainy, testX, testy = make_torch_tens_float_mod(this_dir,file_root) \n",
    "                print(f'trainX:{trainX.shape}, train y: {trainy.shape}, testX: {testX.shape}, test y: {testy.shape}')\n",
    "                train_perf_df, test_perf_df = save_results(trainX, trainy, testX, testy,file_root,GP_path)\n",
    "                train_perf_df['NEK'] = f'NEK{nek}_{bi}'\n",
    "                train_perf_df['strategy'] = f'{samp}'\n",
    "                train_perf_df['feat_type'] = f'{feat}'\n",
    "                test_perf_df['NEK'] = f'NEK{nek}_{bi}'\n",
    "                test_perf_df['strategy'] = f'{samp}'\n",
    "                test_perf_df['feat_type'] = f'{feat}'\n",
    "                train_perf_df.to_csv(f'{this_dir}{file_root}_train_GP_matern.csv',index=False) \n",
    "                test_perf_df.to_csv(f'{this_dir}{file_root}_test_GP_matern.csv',index=False) \n",
    "                print() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958505ef-7fd7-4494-a3df-5ffc1298d0bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "GP_path = '/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/undersampler_validation/under_batch2/'\n",
    "train_results = [] \n",
    "test_results = [] \n",
    "neks = ['2', '3', '5', '9']\n",
    "count=0 \n",
    "folds = ['fold1' ,'fold2','fold3', 'fold4', 'fold5'] \n",
    "\n",
    "feat_types = ['moe', 'mfp']\n",
    "for nek in neks:\n",
    "    bind_inhib = ['binding', 'inhibition']\n",
    "    if nek in ['3','5']: \n",
    "        bind_inhib = ['binding']\n",
    "    for bi in bind_inhib: \n",
    "        if bi == 'binding': \n",
    "            this_bi = 'bind' \n",
    "        if bi == 'inhibition': \n",
    "            this_bi = 'inhib'\n",
    "        this_ct = 0 \n",
    "        for feat in ['moe', 'mfp']: \n",
    "            for samp in ['UNDER']: \n",
    "\n",
    "                print(f'NEK{nek} {this_bi} {feat} {samp}')\n",
    "                # fold_dir = f'/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/undersampler_validation/idea5_dir/{fold}/'\n",
    "                file_root = f'NEK{nek}_{bi}_{feat}_{samp}_batch2'\n",
    "                train = pd.read_csv(f'{fold_dir}{file_root}_train_GP_matern_batch2.csv').iloc[0]\n",
    "                test = pd.read_csv(f'{fold_dir}{file_root}_test_GP_matern_batch2.csv').iloc[0]\n",
    "                # NEK2_binding_moe_UNDER_df_fold1_train_GP_matern.csv\n",
    "                train_results.append(train)\n",
    "                test_results.append(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ef80c-d4f1-42eb-b976-513f783f876e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_results, ignore_index=True)\n",
    "test_df = pd.concat(test_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427cf4a-98a6-4424-96c7-51416f287f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = ['model','NEK', 'strategy', 'feat_type','cm','prediction_type', 'recall','ROC-AUC', 'MCC',\n",
    "       'Balanced Accuracy', 'f1', 'accuracy', 'precision',\n",
    "       'specificity', 'TN', 'FN', 'FP','TP']\n",
    "\n",
    "train_df =  pd.DataFrame(train_results,columns=metric_cols)\n",
    "gp_path ='/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/undersampler_validation/idea5_dir/'\n",
    "train_df.to_csv(f'{gp_path}GP_maternkern_train_results_foldval.csv', index=False)\n",
    "test_df =  pd.DataFrame(test_results,columns=metric_cols)\n",
    "test_df.to_csv(f'{gp_path}GP_maternkern_test_results_foldval.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3444a48-209d-4e78-9b4e-22bbd3d03eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# ends = [] \n",
    "# dests = []\n",
    "# for fold in folds: \n",
    "#     # train_end = f'df_{fold}_train_GP_matern.csv'\n",
    "#     # test_end = f'df_{fold}_test_GP_matern.csv'\n",
    "#     # ends.append(train_end)\n",
    "#     # ends.append(test_end)\n",
    "#     # now do with pickle files\n",
    "#     # NEK2_binding_mfp_ADASYN_df_fold1_GP_Dirichlet_matern_likelihood.pkl\n",
    "#     train_end = f'df_{fold}_GP_Dirichlet_matern_likelihood.pkl'\n",
    "#     test_end = f'df_{fold}_GP_Dirichlet_matern_model.pkl'\n",
    "#     ends.append(train_end)\n",
    "#     ends.append(test_end)\n",
    "#     fold_dir = f'/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/undersampler_validation/idea5_dir/{fold}/'\n",
    "#     dests.append(fold_dir)\n",
    "#     dests.append(fold_dir) \n",
    "                 \n",
    "# directory= f'/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/NEK/GP/GP_matern_kernel/' \n",
    "                                       \n",
    "# def search_and_move_files(source_directory, destination_directory, filename_endings):\n",
    "#     moved_files = []\n",
    "#     not_found = filename_endings.copy()  # Start with all endings in `not_found`\n",
    "\n",
    "#     for root, _, files in os.walk(source_directory):\n",
    "#         for file in files:\n",
    "#             # Check if the file ends with any of the filename endings\n",
    "#             for ending in filename_endings:\n",
    "#                 if file.endswith(ending):\n",
    "#                     source_path = os.path.join(root, file)\n",
    "#                     destination_path = os.path.join(destination_directory, file)\n",
    "                    \n",
    "#                     # Ensure destination folder exists\n",
    "#                     os.makedirs(destination_directory, exist_ok=True)\n",
    "                    \n",
    "#                     # Move the file\n",
    "#                     try:\n",
    "#                         shutil.move(source_path, destination_path)\n",
    "#                         moved_files.append(destination_path)\n",
    "#                         not_found.remove(ending)  # Remove from `not_found` if found and moved\n",
    "#                         print(f\"Moved {file} to {destination_directory}\")\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error moving {file}: {e}\")\n",
    "#                     break  # Move to the next file once a match is found\n",
    "                    \n",
    "#     # Report missing files\n",
    "#     if not_found:\n",
    "#         print(\"Files not found:\", not_found)\n",
    "    \n",
    "#     return moved_files, not_found\n",
    "\n",
    "# # Run the combined search and move function for each ending and its destination\n",
    "# for end, dest in zip(ends, dests): \n",
    "#     moved_files, not_found_files = search_and_move_files(directory, dest, [end])\n",
    "#     print(\"Moved files:\", moved_files)\n",
    "#     if not_found_files:\n",
    "#         print(\"Files not found:\", not_found_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87ac92-d2e8-48bc-9e5a-fadbd0d85356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55abaa-671c-4823-bc58-1a6a61964dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1633c42f-de62-4bc9-bbad-0e7ccb816e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "1512+112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e7b5b-d1ce-41b5-abf1-87efcf2c487d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ee53c-f735-4673-9418-381eb7d8dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df['NEK'] == 'NEK2_inhibition']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "152d06b4-735f-40d5-ba36-7379db7e268e",
   "metadata": {},
   "source": [
    "11. NEK2 inhib moe SMOTE\n",
    "3. NEK2_inhibition_moe_SMOTE\n",
    "(29,) (29,)\n",
    "mean_pred_class0                                            -0.599921\n",
    "mean_pred_class1                                            -5.646675\n",
    "y                                                                 0.0\n",
    "y_pred                                                              0\n",
    "var_pred_class0                                              2.328294\n",
    "var_pred_class1                                              6.429068\n",
    "pred_prob_class0                                              0.99296\n",
    "pred_prob_class1                                              0.00704\n",
    "pred_prob_std_class0                                         0.003808\n",
    "pred_prob_std_class1                                         0.003808\n",
    "model                   NEK2_inhibition_moe_SMOTE_GP_Dirichlet_matern\n",
    "subset                                                          train\n",
    "cm                                                 [1511, 1, 0, 1512]\n",
    "prediction_type     \n",
    "TN\n",
    "ROC-AUC                                                      0.999669\n",
    "MCC                                                          0.999339\n",
    "Balanced Accuracy                                            0.999669\n",
    "f1                                                           0.999669\n",
    "accuracy                                                     0.999669\n",
    "precision                                                    0.999339\n",
    "recall                                                            1.0\n",
    "specificity                                                  0.999339\n",
    "TN                                                               1511\n",
    "FN                                                                  0\n",
    "FP                                                                  1\n",
    "TP                                                               1512\n",
    "NEK                                                      NEK2_binding\n",
    "strategy                                                        SMOTE\n",
    "feat_type                                                         moe\n",
    "Name: 0, dtype: object"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f897f0b1-4ec2-48e6-8ac0-2f02fbd0c672",
   "metadata": {},
   "source": [
    "1. NEK9_inhibition_moe_scaled\n",
    "(29,) (29,)\n",
    "mean_pred_class0                                             -0.450802\n",
    "mean_pred_class1                                             -6.501885\n",
    "y                                                                  0.0\n",
    "y_pred                                                               0\n",
    "var_pred_class0                                               0.692029\n",
    "var_pred_class1                                               6.333593\n",
    "pred_prob_class0                                               0.99497\n",
    "pred_prob_class1                                               0.00503\n",
    "pred_prob_std_class0                                          0.009783\n",
    "pred_prob_std_class1                                          0.009783\n",
    "model                   NEK9_inhibition_moe_scaled_GP_Dirichlet_matern\n",
    "subset                                                           train\n",
    "cm                                                     [280, 0, 33, 0]\n",
    "prediction_type                                                     TN\n",
    "ROC-AUC                                                            0.5\n",
    "MCC                                                                0.0\n",
    "Balanced Accuracy                                                  0.5\n",
    "f1                                                                 0.0\n",
    "accuracy                                                      0.894569\n",
    "precision                                                          0.0\n",
    "recall                                                             0.0\n",
    "specificity                                                        1.0\n",
    "TN                                                                 280\n",
    "FN                                                                  33\n",
    "FP                                                                   0\n",
    "TP                                                                   0\n",
    "NEK                                                    NEK9_inhibition\n",
    "strategy                                                        scaled\n",
    "feat_type                                                          moe\n",
    "Name: 0, dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d838a3-0433-46e7-ae40-3bb97421c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_results2 = [] \n",
    "test_results2 = [] \n",
    "neks = ['9']\n",
    "count=0 \n",
    "samplings = ['scaled', 'UNDER', 'SMOTE', 'ADASYN'] \n",
    "feat_types = ['moe', 'mfp']\n",
    "for nek in neks:\n",
    "    \n",
    "    bi ='inhibition'\n",
    "    this_bi = 'inhib'\n",
    "    this_ct = 0 \n",
    "    for feat in feat_types: \n",
    "        for samp in samplings: \n",
    "            count+=1 \n",
    "            print(f'{count}. NEK{nek} {this_bi} {feat} {samp}')\n",
    "            file_root = f'NEK{nek}_{bi}_{feat}_{samp}'\n",
    "            train = pd.read_csv(f'{GP_path_matern}{file_root}_train_GP_matern.csv').iloc[0]\n",
    "            test = pd.read_csv(f'{GP_path_matern}{file_root}_test_GP_matern.csv').iloc[0]\n",
    "            \n",
    "            display(pd.read_csv(f'{GP_path_matern}{file_root}_train_GP_matern.csv').head(2))\n",
    "            # if (nek == '9' and bi == 'inhibition'): \n",
    "            #     this_ct +=1 \n",
    "            #     print(str(this_ct)+'. '+file_root)\n",
    "            #     print(train.shape, test.shape)\n",
    "            #     # display(test)\n",
    "            #     print(train)\n",
    "            #     print()\n",
    "            nek9_inhib_cols = pd.read_csv(f'{GP_path_matern}{file_root}_train_GP_matern.csv').columns\n",
    "            train_results2.append(train)\n",
    "            test_results2.append(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e472eb-9655-4be6-9b2d-71ad5b596d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nek2_inhib_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5296a-9b8b-4dfa-a901-2a75465186f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nek9_inhib_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ca81b-13e5-42f8-9b41-3ca42d196e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "nek2_inhib_cols == nek9_inhib_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c0b76-3ba2-424a-9b22-73587f349f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch-venv",
   "language": "python",
   "name": "gpytorch-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
