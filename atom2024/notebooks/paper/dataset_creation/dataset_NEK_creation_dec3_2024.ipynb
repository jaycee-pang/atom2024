{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from imblearn.over_sampling import SMOTEN, ADASYN, SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_name</th>\n",
       "      <th>compound_id</th>\n",
       "      <th>base_rdkit_smiles</th>\n",
       "      <th>pct_binding</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_10</td>\n",
       "      <td>CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)c...</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_100</td>\n",
       "      <td>C[C@@H](Oc1cc(C(=O)Nc2ccc(C(=O)N3CCN(C)CC3)cc2...</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_1003</td>\n",
       "      <td>CC1(O)CC(c2nc(-c3ccc4ccc(-c5ccccc5)nc4c3)c3c(N...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_101</td>\n",
       "      <td>COCC(=O)NC/C=C/c1ccc2ncnc(Nc3ccc(Oc4ccc(C)nc4)...</td>\n",
       "      <td>6.158358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_102</td>\n",
       "      <td>COC(=O)c1ccc2c(c1)NC(=O)/C2=C(\\Nc1ccc(N(C)C(=O...</td>\n",
       "      <td>6.158358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_976</td>\n",
       "      <td>Oc1c2ccccc2c2c3c(cccc13)N=N2</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_98</td>\n",
       "      <td>Cc1nc(Nc2n[nH]c3c2CN(C(=O)N[C@H](CN(C)C)c2cccc...</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_980</td>\n",
       "      <td>Cc1cnc(-c2cnc(NCCNc3ccc(C#N)cn3)nc2-c2ccc(Cl)c...</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_986</td>\n",
       "      <td>Nc1n[nH]c2nnc(-c3ccccc3)c(-c3ccccc3)c12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>NEK2</td>\n",
       "      <td>kdb_99</td>\n",
       "      <td>C[C@@H](Oc1cc(-n2cnc3ccc(CN4CCN(C)CC4)cc32)sc1...</td>\n",
       "      <td>79.365079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1408 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     target_name compound_id  \\\n",
       "0           NEK2      kdb_10   \n",
       "1           NEK2     kdb_100   \n",
       "2           NEK2    kdb_1003   \n",
       "3           NEK2     kdb_101   \n",
       "4           NEK2     kdb_102   \n",
       "...          ...         ...   \n",
       "1403        NEK2     kdb_976   \n",
       "1404        NEK2      kdb_98   \n",
       "1405        NEK2     kdb_980   \n",
       "1406        NEK2     kdb_986   \n",
       "1407        NEK2      kdb_99   \n",
       "\n",
       "                                      base_rdkit_smiles  pct_binding  active  \n",
       "0     CCS(=O)(=O)N1CC(CC#N)(n2cc(-c3ncnc4[nH]ccc34)c...     3.225806       0  \n",
       "1     C[C@@H](Oc1cc(C(=O)Nc2ccc(C(=O)N3CCN(C)CC3)cc2...     3.225806       0  \n",
       "2     CC1(O)CC(c2nc(-c3ccc4ccc(-c5ccccc5)nc4c3)c3c(N...     5.000000       0  \n",
       "3     COCC(=O)NC/C=C/c1ccc2ncnc(Nc3ccc(Oc4ccc(C)nc4)...     6.158358       0  \n",
       "4     COC(=O)c1ccc2c(c1)NC(=O)/C2=C(\\Nc1ccc(N(C)C(=O...     6.158358       0  \n",
       "...                                                 ...          ...     ...  \n",
       "1403                       Oc1c2ccccc2c2c3c(cccc13)N=N2     9.090909       0  \n",
       "1404  Cc1nc(Nc2n[nH]c3c2CN(C(=O)N[C@H](CN(C)C)c2cccc...     3.225806       0  \n",
       "1405  Cc1cnc(-c2cnc(NCCNc3ccc(C#N)cn3)nc2-c2ccc(Cl)c...    85.000000       1  \n",
       "1406            Nc1n[nH]c2nnc(-c3ccccc3)c(-c3ccccc3)c12     0.000000       0  \n",
       "1407  C[C@@H](Oc1cc(-n2cnc3ccc(CN4CCN(C)CC4)cc32)sc1...    79.365079       1  \n",
       "\n",
       "[1408 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../datasets/original_datasets/'\n",
    "test = pd.read_csv(data_path+'NEK2_1_uM_min_50_pct_binding.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(data_path, filename): \n",
    "  \n",
    "    df = pd.read_csv(data_path+filename)\n",
    "\n",
    "    pct_col = [col for col in df.columns if col.startswith('pct_')] \n",
    "    len(pct_col) == 1 \n",
    "    pct_col = pct_col[0]\n",
    "    invalid_labels = df[df['active']!= (df[pct_col]>=50).astype(int)]\n",
    "    if invalid_labels.empty: \n",
    "        # print(f'correct label assignment based on {pct_col} column')\n",
    "        pass\n",
    "    else: \n",
    "        print(f'Error on label assignement.')\n",
    "        print(invalid_labels)\n",
    "    # check for duplicates here \n",
    "    duplicates = df[df.duplicated()]\n",
    "    print(f'{df.shape}')\n",
    "    #duplicates = df[df.duplicated(subset=['base_rdkit_smiles'])]\n",
    "   \n",
    "    if not duplicates.empty:\n",
    "        print('Duplicate rows found:')\n",
    "        print(duplicates)\n",
    "        df_cleaned = df.drop_duplicates()\n",
    "        print(f'df size original: {df.shape}')\n",
    "        print(f'remove duplicates. {df_cleaned.shape}')\n",
    "check_labels(data_path,'NEK2_1_uM_min_50_pct_inhibition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset with folds\n",
    "\n",
    "# must specify filename because the 50pct binding and inhibition files differ\n",
    "# to-do: discuss how to do this --> this will create a file that is just this but with column specifying split\n",
    "#       do we want to already integrate a split_uuid like AMPL? \n",
    "#       additionally, we still have to use the splits to create the \"scaled df\" \n",
    "def remove_duplicates(datapath, filename):\n",
    "    df = pd.read_csv(f'{datapath}{filename}')\n",
    "    duplicates = df.duplicated(subset=['base_rdkit_smiles'])\n",
    "    if duplicates.any():\n",
    "        print('Duplicate rows found:')\n",
    "        print(df[duplicates])\n",
    "        df_cleaned = df.drop_duplicates(subset=['base_rdkit_smiles'])\n",
    "        print(f'df size original: {df.shape}')\n",
    "        print(f'Removed duplicates. New df size: {df_cleaned.shape}')\n",
    "    else: \n",
    "        df_cleaned = df\n",
    "    return df_cleaned\n",
    "    \n",
    "def create_folds(datapath, filename, k_splits): \n",
    "    \"\"\"filename: NEK#_1_uM_min_50_pct_(binding or inhibition).csv\"\"\"\n",
    "    df = remove_duplicates(datapath, filename)\n",
    "    # is this extra/too much and should we just assume what labels coincide with majority and minority? \n",
    "    # determine majority/minority class \n",
    "    #       AMPL takes in 'response column' \n",
    "    class_labels = df['active'].value_counts() \n",
    "    print(class_labels)\n",
    "    if len(class_labels)>1: \n",
    "        majority_class_label =class_labels.idxmax() \n",
    "        majority_num = class_labels.max() \n",
    "        minority_class_label = class_labels.idxmin()\n",
    "        minority_count = class_labels.min()\n",
    "    df_majority = df[df['active']==majority_class_label]\n",
    "    df_minority=df[df['active']==minority_class_label]\n",
    "    # copy to avoid warnings \n",
    "    df_majority = df_majority.copy()\n",
    "    df_minority = df_minority.copy()\n",
    "    kf = KFold(k_splits,shuffle=True, random_state=42)\n",
    "    # majority \n",
    "    for i, (_, v_ind) in enumerate(kf.split(df_majority)):\n",
    "        df_majority.loc[df_majority.index[v_ind], 'fold'] = f\"fold{i+1}\"\n",
    "    # minority \n",
    "    for i, (_, v_ind) in enumerate(kf.split(df_minority)):\n",
    "        df_minority.loc[df_minority.index[v_ind], 'fold'] = f\"fold{i+1}\"\n",
    "    all_fold_df = pd.concat([df_majority,df_minority])\n",
    "    \n",
    "    print(all_fold_df.shape)\n",
    "    print(all_fold_df.active.value_counts())\n",
    "    ## actually, this might be perfect jsut to save as \"split df\" because then \n",
    "    # in the next step of creating a dataset, you can choose what fold you want to be the test set \n",
    "    return all_fold_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and__(data_path, fold_df, test_fold):\n",
    "   \"\"\"function to label 'train' or 'test' in the 'subset' column\n",
    "   to be used to create train/test OR train/val\n",
    "   fold_df: dataframe with column 'fold'\n",
    "   test_fold (str): fold to make the test set (the remaining folds will be train)\n",
    "   \"\"\"\n",
    "   df = pd.read_csv(data_path+fold_df) \n",
    "   df['subset'] = df['fold'].apply(lambda x: 'test' if x == test_fold else 'train')\n",
    "\n",
    "   return df[['NEK','compound_id', 'active', 'base_rdkit_smiles', 'subset']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bind_inhib(df_name):\n",
    "    match = re.search(r'_([^_]+)\\.csv$',df_name)\n",
    "    return match.group(1) if match else None\n",
    "    # returns 'binding' or 'inhibition' \n",
    "og_dfs = [file for file in os.listdir(data_path) if file.startswith('NEK')]\n",
    "\n",
    "for df in og_dfs: \n",
    "    folded_df = create_folds(data_path, df,5)\n",
    "    bind_inhib = get_bind_inhib(df)\n",
    "    nek_name = f'NEK{df[3]}_{bind_inhib}'\n",
    "    print(nek_name)\n",
    "    folded_df['NEK']=nek_name \n",
    "    folded_df.to_csv(f'../datasets/initial_fold/{nek_name}_initial_fold.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neks = ['NEK2_binding', 'NEK2_inhibition', 'NEK3_binding', 'NEK5_binding','NEK9_binding','NEK9_inhibition']\n",
    "\n",
    "for nek in neks:\n",
    "    fold_df_name = nek+\"_initial_fold.csv\"\n",
    "    print(fold_df_name)\n",
    "    labeled_df = train_and__(\"/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/initial_fold/\" , fold_df_name, \"fold1\")\n",
    "    labeled_df.to_csv(f'/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/{nek}_80_20_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featurize \n",
    "def featurize(data_path, filename, feat_type, moe_path=None, moe_file=None, mfp_radius=2, nBits=2048): \n",
    "    if (feat_type == 'MOE') and (moe_path is not None) and (moe_file is not None): \n",
    "        df = create_moe(data_path, filename, moe_path, moe_file)\n",
    "    elif feat_type == 'MFP': \n",
    "        df = create_mfp(data_path, filename, mfp_radius, nBits)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moe(data_path, filename, moe_path, moe_file):\n",
    "    \"\"\"(intended use for already existing dataset)\n",
    "    This function will use an existing dataframe with smiles column to\n",
    "    get the features from an existing file (moe_path+moe_file) with the MOE features generated\"\"\"\n",
    "    \n",
    "    drop_cols = ['active', 'compound_id']\n",
    "    df = remove_duplicates(data_path, filename)\n",
    "    df=df.drop(columns=drop_cols)\n",
    "    \n",
    "\n",
    "    moe_df=remove_duplicates(moe_path,moe_file)\n",
    "    \n",
    "\n",
    "    final_df=moe_df.merge(df, how='outer', on=['base_rdkit_smiles'], suffixes=('_moe_desc', '_og'))\n",
    "    id_cols = ['NEK', 'compound_id','base_rdkit_smiles','subset', 'active']\n",
    "    feat_cols = set(list(final_df.columns))-set(id_cols)\n",
    "    \n",
    "    final_order_cols = list(id_cols)+list(feat_cols)\n",
    "\n",
    "\n",
    "    return final_df[final_order_cols]\n",
    "    # in production code , check for smiles col \n",
    "\n",
    "test_df = featurize('/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/','NEK2_inhibition_80_20_df.csv', 'MOE','/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/original_datasets/scaled_descriptors/' , 'NEK2_1_uM_min_50_pct_inhibition_with_moe_descriptors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_fps(smiles_list, radius=2, nBits=2048):\n",
    "    fps = []\n",
    "    for smi in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is not None:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "            arr = np.zeros((1,), dtype=np.int8)\n",
    "            Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "            fps.append(arr)\n",
    "    return np.array(fps)\n",
    "\n",
    "\n",
    "def create_mfp(file_path, filename, mfp_radius, nBits):\n",
    "    df = pd.read_csv(file_path+filename)\n",
    "\n",
    "    id_data = df[['compound_id','base_rdkit_smiles', 'NEK','subset','active']]\n",
    "    smiles = df['base_rdkit_smiles']\n",
    "\n",
    "    mfp_feats = smiles_to_fps(smiles,mfp_radius,nBits)\n",
    "    mfp_df = pd.DataFrame(mfp_feats)\n",
    "\n",
    "    id_cols = ['NEK', 'compound_id','base_rdkit_smiles','subset', 'active']\n",
    "    feat_cols = set(list(mfp_df.columns))-set(id_cols)\n",
    "    final_order_cols = list(id_cols)+list(feat_cols)\n",
    "    final_df = pd.concat([mfp_df, df],axis=1)\n",
    "    final_df = final_df[final_order_cols]\n",
    "    #display(final_df)\n",
    "\n",
    "    \n",
    "    # testX_mfp_df['active'] = test_y.reset_index(drop=True)\n",
    "    # final_df = pd.concat([trainX_mfp_df,testX_mfp_df], ignore_index=True)\n",
    "    return final_df\n",
    "        \n",
    "\n",
    "test_df = featurize('/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/','NEK2_inhibition_80_20_df.csv', 'MFP',2,2048 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_df_path = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/' \n",
    "scaled_desc_path = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/original_datasets/scaled_descriptors/'\n",
    "featurized_filepath = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/' \n",
    "for nek in ['2','3','5','9']: \n",
    "    if nek in ['2','9']:\n",
    "        bind_inhib = ['binding','inhibition']\n",
    "    else: \n",
    "        bind_inhib = ['binding']\n",
    "    for bi in bind_inhib: \n",
    "\n",
    "        for feat in ['MOE','MFP']: \n",
    "            print(f'NEK {nek} {bi} {feat}')\n",
    "            og_df_name = f'NEK{nek}_{bi}_80_20_df.csv'\n",
    "            if feat == 'MOE': \n",
    "                desc_filename =f'NEK{nek}_1_uM_min_50_pct_{bi}_with_moe_descriptors.csv'\n",
    "                final_moe_df = featurize(og_df_path,og_df_name,feat,scaled_desc_path,desc_filename)\n",
    "                final_moe_df.to_csv(f'{featurized_filepath}NEK{nek}_{bi}_{feat}_none_unscaled.csv',index=False)\n",
    "                \n",
    "            elif feat=='MFP': \n",
    "                final_mfp_df = featurize(og_df_path,og_df_name,feat,2,2048)\n",
    "                final_mfp_df.to_csv(f'{featurized_filepath}NEK{nek}_{bi}_{feat}_none_unscaled.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating scalers for eack Nek MOE feats\n",
    "id_cols = ['NEK', 'compound_id','base_rdkit_smiles','subset', 'active']\n",
    "\n",
    "featurized_filepath = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/' \n",
    "scaler_dir = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/scalers/'\n",
    "for nek in neks:  \n",
    "   for feat in ['MOE']: \n",
    "      \n",
    "        df = pd.read_csv(f'{featurized_filepath}{nek}_{feat}_none_unscaled.csv')\n",
    "        feat_cols = set(list(df.columns))-set(id_cols)\n",
    "        train_x = df[df[\"subset\"] == \"train\"]\n",
    "        train_id = train_x[id_cols]\n",
    "        train_x.drop(columns = id_cols, inplace=True)\n",
    "        test_x = df[df[\"subset\"] == \"test\"]\n",
    "        test_id = test_x[id_cols]\n",
    "        test_x.drop(columns = id_cols, inplace=True)\n",
    "        \n",
    "        scaling = StandardScaler()\n",
    "        scaling.fit(train_x)\n",
    "        #save to pickle\n",
    "        with open(f'{scaler_dir}_{nek}_{feat}_scaler.pkl', 'wb') as f: \n",
    "            pickle.dump(scaling, f) \n",
    "        train_scaled = scaling.transform(train_x)\n",
    "        test_scaled = scaling.transform(test_x)\n",
    "        trainX_final = pd.DataFrame(train_scaled, columns=list(feat_cols))\n",
    "        testX_final = pd.DataFrame(test_scaled, columns=list(feat_cols))\n",
    "        scaled_data = pd.concat([trainX_final, testX_final])\n",
    "        id_data = pd.concat([train_id,test_id])\n",
    "        no_samp_data_scaled = pd.concat([id_data.reset_index(drop=True), scaled_data.reset_index(drop=True)], axis=1)\n",
    "        no_samp_data_scaled.to_csv(f'{featurized_filepath}{nek}_{feat}_none_scaled.csv',index=False)\n",
    "\n",
    "# MFP feats are inherently scaled\n",
    "for nek in neks: \n",
    "   for feat in ['MFP']: \n",
    "      \n",
    "        df = pd.read_csv(f'{featurized_filepath}{nek}_{feat}_none_unscaled.csv')\n",
    "        df.to_csv(f'{featurized_filepath}{nek}_{feat}_none_scaled.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['NEK', 'compound_id', 'base_rdkit_smiles','subset', 'active'] \n",
    "def over_sampling(data_path,filename, sampling):\n",
    "    \"\"\"Oversample the datasetes using the SMOTE or ADASYN\n",
    "    Keeps the feature names and id cols\n",
    "    file_name (full/absolute path): use the scaled dataframe we just created above 'NEK#_(binding/inhibition)_(MOE/MFP)_none_scaled_df.csv'\n",
    "    sampling (str): 'SMOTE' or 'ADASYN'\n",
    "    returns: oversampled dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_path+filename) # this is the already scaled ver\n",
    "    feat_cols = list(set(list(df.columns))-set(id_cols))\n",
    "    \n",
    "    # train and test \n",
    "    train = df[df['subset']=='train'] \n",
    "    test =df[df['subset']=='test'] \n",
    "    nek = df['NEK'].iloc[0]\n",
    "    print(f\"nek: {nek}, {type(nek)}\")\n",
    "    # separate just id cols\n",
    "    just_ids = ['NEK', 'compound_id', 'base_rdkit_smiles','subset']\n",
    "    train_just_ids = train[just_ids]\n",
    "    test_just_ids = test[just_ids]\n",
    "\n",
    "    # just feats and 'active'\n",
    "    trainX = train[feat_cols]\n",
    "    testX = test[feat_cols]\n",
    "    \n",
    "    trainy = train['active']\n",
    "    testy = test['active']\n",
    "    \n",
    "    if sampling == 'ADASYN':\n",
    "        oversample = ADASYN(random_state=42)\n",
    "    else: \n",
    "        oversample = SMOTE(random_state=42)\n",
    "\n",
    "    \n",
    "    trainX_temp, trainy_temp = oversample.fit_resample(trainX.to_numpy(), trainy.to_numpy().reshape(-1))\n",
    "    print(f'train after {sampling}: {trainX_temp.shape}')\n",
    "    \n",
    "    trainX_resamp = pd.DataFrame(trainX_temp, columns=feat_cols)\n",
    "    trainy_resamp = pd.DataFrame(trainy_temp, columns=['active'])\n",
    "\n",
    "    num_real = len(train)\n",
    "    num_synthetic = len(trainX_resamp)-num_real\n",
    "    synthetic_ids = pd.DataFrame({'NEK': [nek] * num_synthetic,\n",
    "        'compound_id': [f'synthetic_{sampling}_{i}' for i in range(num_synthetic)],\n",
    "        'base_rdkit_smiles': [f'synthetic_{sampling}'] * num_synthetic,\n",
    "        'subset': ['train']*num_synthetic}) # ,'active':[1]*num_synthetic}\n",
    "\n",
    "    real_ids = train_just_ids.reset_index(drop=True)\n",
    "    combined_ids = pd.concat([real_ids,synthetic_ids], ignore_index=True)\n",
    "    \n",
    "    train_resamp = pd.concat([combined_ids, trainX_resamp, trainy_resamp[['active']]], axis=1)\n",
    "\n",
    "    print(train_resamp.columns[train_resamp.columns.duplicated()])\n",
    "    test_df_final = pd.concat([test_just_ids.reset_index(drop=True),\n",
    "                               testX.reset_index(drop=True), testy.reset_index(drop=True)],axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    # test_df_final['subset'] = 'test'\n",
    "    # train_resamp = train_resamp.reindex(columns=df.columns)\n",
    "    # test_df_final = test_df_final.reindex(columns=df.columns)\n",
    "    \n",
    "    \n",
    "    final_df = pd.concat([train_resamp, test_df_final]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    return final_df[list(df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#over_sampling(featurized_filepath, 'NEK2_binding_MFP_none_scaled.csv', \"SMOTE\", \"NEK2_binding\")\n",
    "#testing_jaycee = over_sampling('/Users/jayceepang/msse/capstone/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/', 'NEK2_binding_MOE_none.csv', \"SMOTE\", \"NEK2_binding\")\n",
    "testing_rs = over_sampling('/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/', 'NEK2_binding_MFP_none_scaled.csv', \"SMOTE\")\n",
    "\n",
    "print(testing_rs['subset'].value_counts())\n",
    "print(testing_rs[testing_rs['subset']=='train']['active'].value_counts())\n",
    "print(testing_rs[testing_rs['subset']=='test']['active'].value_counts())\n",
    "testing_rs[testing_rs[\"base_rdkit_smiles\"] == \"synthetic_SMOTE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['NEK', 'compound_id', 'base_rdkit_smiles','subset', 'active'] \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "def under_sampling(path,filename): \n",
    "    df = pd.read_csv(path+filename) # this is the already scaled ver\n",
    "    feat_cols = list(set(list(df.columns))-set(id_cols))\n",
    "    \n",
    "    # train and test \n",
    "    train = df[df['subset']=='train'] \n",
    "    test =df[df['subset']=='test'] \n",
    "\n",
    "    # separate just id cols\n",
    "    just_ids = ['NEK', 'compound_id', 'base_rdkit_smiles','subset']\n",
    "    train_just_ids = train[just_ids]\n",
    "    test_just_ids = test[just_ids]\n",
    "\n",
    "    # just feats and 'active'\n",
    "    trainX = train[feat_cols]\n",
    "    testX = test[feat_cols]\n",
    "    \n",
    "    trainy = train['active']\n",
    "    testy = test['active']\n",
    "    \n",
    "    undersample = RandomUnderSampler(random_state=42)\n",
    "    \n",
    "    trainX_temp, trainy_temp = undersample.fit_resample(trainX.to_numpy(), trainy.to_numpy().reshape(-1))\n",
    "    \n",
    "    trainX_resamp = pd.DataFrame(trainX_temp, columns=feat_cols)\n",
    "    trainy_resamp = pd.DataFrame(trainy_temp, columns=['active'])\n",
    "    \n",
    "    train_ids_resamp = train_just_ids.iloc[trainX_resamp.index].reset_index(drop=True)\n",
    "    train_resamp= pd.concat([train_ids_resamp, trainX_resamp,trainy_resamp], axis=1)\n",
    "    # train_resamp['subset'] = 'train'\n",
    "\n",
    "    test_df_final = pd.concat([test_just_ids.reset_index(drop=True),testX.reset_index(drop=True),testy.reset_index(drop=True)],axis=1)\n",
    "    # test_df_final['subset'] = 'test'\n",
    "    final_df = pd.concat([train_resamp,test_df_final]).reset_index(drop=True)\n",
    "    return final_df[list(df.columns)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_rs_under = under_sampling('/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/', 'NEK2_binding_MFP_none_scaled.csv')\n",
    "\n",
    "print(testing_rs_under['subset'].value_counts())\n",
    "print(testing_rs_under[testing_rs_under['subset']=='train']['active'].value_counts())\n",
    "print(testing_rs_under[testing_rs_under['subset']=='test']['active'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['NEK', 'compound_id', 'base_rdkit_smiles','subset', 'active'] ### MAKE SURE FOLD IS NOT IN THIS \n",
    "filepath = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/'\n",
    "\n",
    "def create_arrays(filepath, root,save_path, printout=True): \n",
    "    df = pd.read_csv(filepath+root+\".csv\")\n",
    "    train = df[df['subset']=='train']\n",
    "    test=df[df['subset']=='test']\n",
    "    feat_cols = list(set(list(df.columns))-set(id_cols))\n",
    "    \n",
    "    train_y = train['active'].to_numpy().reshape(-1)\n",
    "    test_y=test['active'].to_numpy().reshape(-1)\n",
    "    trainX = train.drop(columns=id_cols).to_numpy()\n",
    "    testX = test.drop(columns=id_cols).to_numpy()\n",
    "    if printout: \n",
    "        print(f'train X shape: {trainX.shape}, y: {train_y.shape}, test X: {testX.shape}, y:{test_y.shape}')\n",
    "    if save_path is not None: \n",
    "        trainX = pd.DataFrame(trainX)\n",
    "        trainX.to_csv(filepath+root+'_trainX.csv', index=False)\n",
    "   \n",
    "        trainy_df = pd.DataFrame(train_y)\n",
    "        trainy_df.to_csv(filepath+root+'_train_y.csv', index=False) \n",
    "\n",
    "        testX = pd.DataFrame(testX).iloc[:,:-1]\n",
    "        testX.to_csv(filepath+root+'_testX.csv', index=False)\n",
    "\n",
    "        testy_df = pd.DataFrame(test_y)\n",
    "        testy_df.to_csv(filepath+root+'_test_y.csv', index=False) \n",
    "        \n",
    "    return trainX, train_y, testX, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/'\n",
    "feats = [\"MOE\", \"MFP\"]\n",
    "for nek in neks: \n",
    "    for feat in feats: \n",
    "        for samp in ['UNDER', 'SMOTE', 'ADASYN', \"none_scaled\"]:\n",
    "            og_df_name = f'{nek}_{feat}_none_scaled.csv'\n",
    "            #print(f\"feat:{feat}, samp:{samp}, nek:{nek}\")\n",
    "            if samp == 'UNDER': \n",
    "                sampled_df = under_sampling(filepath,og_df_name) \n",
    "                sampled_df.to_csv(f'{filepath}{nek}_{feat}_{samp}.csv',index=False)\n",
    "                print(f\"feat:{feat}, samp:{samp}, nek:{nek}\")\n",
    "            elif samp == \"SMOTE\" or samp == \"ADASYN\": \n",
    "                sampled_df=over_sampling(filepath,og_df_name, samp) \n",
    "                sampled_df.to_csv(f'{filepath}{nek}_{feat}_{samp}.csv',index=False) \n",
    "                #print(f\"feat:{feat}, samp:{samp}, nek:{nek}\")    \n",
    "            root_name = f'{nek}_{feat}_{samp}'\n",
    "            _, _,_,_=create_arrays(filepath, root_name,filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds_train(datapath, filename, k_splits): \n",
    "    \"\"\"filename: NEK#_1_uM_min_50_pct_(binding or inhibition).csv\"\"\"\n",
    "    #df = remove_duplicates(datapath, filename)\n",
    "    df = pd.read_csv(f'{datapath}{filename}')\n",
    "    df = df[df[\"subset\"]== \"train\"]\n",
    "\n",
    "\n",
    "    df['source'] = df['base_rdkit_smiles'].apply(lambda x: 'synthetic' if 'synthetic' in str(x).lower() else 'real')\n",
    "    df['stratify_key'] = df['active'].astype(str) + \"_\" + df['source']\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    df[\"fold\"] = -1\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(skf.split(df, df['stratify_key'])):\n",
    "        df.loc[df.index[test_index],'fold'] = f\"fold{i+1}\"\n",
    "    \n",
    "    remove_cols = [\"source\", 'stratify_key']\n",
    "    df = df.drop(columns=remove_cols)\n",
    "        \n",
    "    #display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_folds_train('/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/', 'NEK2_binding_MFP_ADASYN.csv',5)\n",
    "\n",
    "featurized_path = '/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/featurized/'\n",
    "feats = [\"MOE\", \"MFP\"]\n",
    "for nek in neks: \n",
    "    for feat in feats: \n",
    "        for samp in ['UNDER', 'SMOTE', 'ADASYN', \"none_scaled\"]:\n",
    "            og_df_name = f'{nek}_{feat}_{samp}.csv'\n",
    "            folded_df_val = create_folds_train(featurized_path, og_df_name,5)\n",
    "            type_df = type(folded_df_val)\n",
    "            folded_df_val.to_csv('/Users/radhi/Desktop/GitHub/atom2024/atom2024/notebooks/paper/datasets/80train_20test/k_fold/'+f'{nek}_{feat}_{samp}_kfold.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
